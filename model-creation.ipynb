{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# CarND Behavioral Cloning Project\n",
    "\n",
    "This project is about training a neural network to drive a car on a simulator using data recorded from a humman driver.\n",
    "\n",
    "This notebook will be used to create the model to be used in driving the car on the simulator.\n",
    "\n",
    "The inputs come in three images right, central and left cameras. I decided to use only the center one, because the get good valid values for the left and right images was a bit dificult.\n",
    "\n",
    "The first thing to do is to clean, then oganize the dataset, and save it to a csv file. for posterior use.\n",
    "\n",
    "The file driving_log.csv contains steering angles and the left, right and center images associated to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os.path\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Dataset preparation\n",
    "\n",
    "I first get all records regarding the center image and shufle them.\n",
    "\n",
    "Then I split the data into train 80% and validation 20%\n",
    "\n",
    "Next I save the train and validation array to csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if not(os.path.exists('train.csv') and os.path.exists('validation.csv')):\n",
    "\n",
    "    path_to_replace = \"C:\\\\Users\\\\eduardo\\\\Documents\\\\SelfDrivingCar\\\\beta-simulator-windows\\\\beta_simulator_windows\\\\data\"\n",
    "\n",
    "    def ReplaceWrongPath(value):\n",
    "        return value.replace(path_to_replace, \"\").replace(\"\\\\\", \"/\").replace(\" \", \"\")\n",
    "\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    X_train_left = []\n",
    "    y_train_left = []\n",
    "    X_train_right = []\n",
    "    y_train_right = []\n",
    "\n",
    "    with open('./data/driving_log.csv', 'r') as csv_file_in:\n",
    "\n",
    "        csv_reader = csv.DictReader(csv_file_in)\n",
    "\n",
    "        for row in csv_reader:\n",
    "            steering = float(row['steering'])\n",
    "\n",
    "            #center image\n",
    "            path = './data/' + ReplaceWrongPath(row['center'].strip())        \n",
    "            X_train.append(path)\n",
    "            y_train.append(steering)\n",
    "\n",
    "    X_train, y_train = shuffle(X_train, y_train)\n",
    "    X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "    [X_train.append(item) for item in X_train_left]\n",
    "    [X_train.append(item) for item in X_train_right]\n",
    "    [y_train.append(item) for item in y_train_left]\n",
    "    [y_train.append(item) for item in y_train_right]\n",
    "    \n",
    "    X_train, y_train = shuffle(X_train, y_train)\n",
    "\n",
    "    with open('train.csv', 'w') as csv_file_train:\n",
    "\n",
    "        fieldnames = ['path','steering']\n",
    "        writer = csv.DictWriter(csv_file_train, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for i in range(len(X_train)):\n",
    "            path, steering = X_train[i], y_train[i]\n",
    "            writer.writerow({'path': path, 'steering': steering})\n",
    "\n",
    "    with open('validation.csv', 'w') as csv_file_train:\n",
    "\n",
    "        fieldnames = ['path','steering']\n",
    "        writer = csv.DictWriter(csv_file_train, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for i in range(len(X_validation)):\n",
    "            path, steering = X_validation[i], y_validation[i]\n",
    "            writer.writerow({'path': path, 'steering': steering})\n",
    "\n",
    "    print(\"processing done\")\n",
    "else:\n",
    "    print(\"files exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Load data \n",
    "\n",
    "In this step the csv files into memory, they are already pre shufled and split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "    \n",
    "    \n",
    "break_classes = 21\n",
    "X_train_left = None\n",
    "y_train_left = None\n",
    "X_train_right = None\n",
    "y_train_right = None\n",
    "\n",
    "X_train= []\n",
    "y_train = []\n",
    "\n",
    "with open('train.csv', 'r') as csv_file_train:\n",
    "    \n",
    "    csv_reader = csv.DictReader(csv_file_train)\n",
    "\n",
    "    for row in csv_reader:\n",
    "        X_train.append(row['path'])\n",
    "        y_train.append(float(row['steering']))\n",
    "        \n",
    "\n",
    "X_validation = []\n",
    "y_validation = []\n",
    "\n",
    "with open('validation.csv', 'r') as csv_file_val:\n",
    "    \n",
    "    csv_reader = csv.DictReader(csv_file_val)\n",
    "\n",
    "    for row in csv_reader:\n",
    "        X_validation.append(row['path'])\n",
    "        y_validation.append(float(row['steering']))\n",
    "        \n",
    "print('Loading done')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Explore data\n",
    "\n",
    "Here I do histogram on the training target, this is also how I try to check if the recorded data is balanced enough, to produce a balanced solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.hist(y_train, bins=70)\n",
    "plt.title(\"Training data\")\n",
    "plt.xlabel(\"Angles\")\n",
    "plt.ylabel(\"# of examples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Model\n",
    "\n",
    "For the model I decided to experiment with the VGG16 pre-trained model.\n",
    "\n",
    "By using a pre-trained VGG16 I could get a working model in just one single epoch.\n",
    "\n",
    "I removed the top fully connected layer, and placed a 4 layers fully connect NN.\n",
    "\n",
    "But I also experimented with other models, that also performed well but not as well as the VGG.\n",
    "\n",
    "Relu layers were used to introduce non linearity.\n",
    "\n",
    "## Chalenges\n",
    "\n",
    "### Data Colection\n",
    "\n",
    "For me the bigest chalenge was collecting the right data to train the model.\n",
    "\n",
    "I had to redo the dataset a few times either because sometimes I had too much recovery examples or too little recovery examples. But in the begining this was a major set back because I thought that the model was the problem, when in reality the problem was the incredibly imbalanced data that I had.\n",
    "\n",
    "My Dataset is composed of:\n",
    "- Two center driving laps on the correct direction.\n",
    "- Two center driving laps on the wrong direction.\n",
    "- One lap revering from the right on the correct direction\n",
    "- One lap revering from the right on the wrong direction\n",
    "- One lap revering from the left on the correct direction\n",
    "- One lap revering from the left on the wrong direction\n",
    "- Two other trys on the curves\n",
    "\n",
    "### Memory limit\n",
    "\n",
    "I also faced a few issues with memory.\n",
    "\n",
    "First it's not possible to load the entire dataset into memmory, so a fit generator was used to create the train and validation set, I also flipped the image horizontaly doubling the dataset size, also in the hope of introducing some generalization with it.\n",
    "\n",
    "Second I had to use smaler batch sizes, because tha GPU that I was using couldn't handle much data.\n",
    "\n",
    "## Croping\n",
    "\n",
    "The original image has a shape of (160, 320, 3) than I've croped 60px from the top and 20px from the bottom.\n",
    "\n",
    "I've also used the Lambda layer after to normalize the image\n",
    "\n",
    "\n",
    "![alt text](./model.png \"Model Visualization\")\n",
    "\n",
    "\n",
    "# Training\n",
    "\n",
    "I chose to use a RMSprop optimizer.\n",
    "\n",
    "And ModelCheckpoint to save the best evaluated model.\n",
    "\n",
    "Tests were done straight into the simulator.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Lambda, Dense, Activation, Flatten, Input, Dropout, Convolution2D, MaxPooling2D, Cropping2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "def get_model():\n",
    "    input_tensor = Input(shape=(160, 320, 3))\n",
    "    croped_input_img = Cropping2D(cropping=((60, 20), (0, 0)))(input_tensor)\n",
    "    croped_input_img = Lambda(lambda x: (x / 255.0) - 0.5)(croped_input_img)\n",
    "    base_model = VGG16(input_tensor=croped_input_img, weights='imagenet', include_top=False)\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    top_model = base_model.output\n",
    "    top_model = Flatten()(top_model)\n",
    "    top_model = Dropout(0.25)(top_model)\n",
    "    \n",
    "    top_model = Dense(2024)(top_model)\n",
    "    top_model = BatchNormalization()(top_model)\n",
    "    top_model = Activation('relu')(top_model)\n",
    "    top_model = Dropout(0.25)(top_model)\n",
    "\n",
    "    top_model = Dense(500)(top_model)\n",
    "    top_model = BatchNormalization()(top_model)\n",
    "    top_model = Activation('relu')(top_model)\n",
    "    top_model = Dropout(0.25)(top_model)\n",
    "\n",
    "    top_model = Dense(200)(top_model)\n",
    "    top_model = BatchNormalization()(top_model)\n",
    "    top_model = Activation('relu')(top_model)\n",
    "    top_model = Dropout(0.25)(top_model)\n",
    "\n",
    "    top_model = Dense(50)(top_model)\n",
    "    top_model = BatchNormalization()(top_model)\n",
    "    top_model = Activation('relu')(top_model)\n",
    "    top_model = Dropout(0.25)(top_model)\n",
    "\n",
    "    predictions = Dense(1)(top_model)\n",
    "\n",
    "    model = Model(input=input_tensor, output=predictions)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sfdp': 'C:\\\\Program Files (x86)\\\\Graphviz2.38\\\\bin\\\\sfdp.exe', 'neato': 'C:\\\\Program Files (x86)\\\\Graphviz2.38\\\\bin\\\\neato.exe', 'fdp': 'C:\\\\Program Files (x86)\\\\Graphviz2.38\\\\bin\\\\fdp.exe', 'twopi': 'C:\\\\Program Files (x86)\\\\Graphviz2.38\\\\bin\\\\twopi.exe', 'dot': 'C:\\\\Program Files (x86)\\\\Graphviz2.38\\\\bin\\\\dot.exe', 'circo': 'C:\\\\Program Files (x86)\\\\Graphviz2.38\\\\bin\\\\circo.exe'}\n",
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n"
     ]
    }
   ],
   "source": [
    "import pydot\n",
    "\n",
    "print(pydot.find_graphviz())\n",
    "\n",
    "from keras.utils.visualize_util import plot\n",
    "\n",
    "model = get_model()\n",
    "\n",
    "plot(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_line(row):\n",
    "    #b,g,r = cv2.split(cv2.imread(row['path']))\n",
    "    #img = np.array(cv2.merge([r,g,b]))\n",
    "    img = cv2.imread(row['path'])\n",
    "    steering = float(row['steering'])\n",
    "    return [img, steering]\n",
    "\n",
    "def generate_arrays_from_file(path, batch_size = 20, flip=True):\n",
    "    while 1:\n",
    "        global X_train\n",
    "        global y_train\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        Xs = []\n",
    "        ys = []        \n",
    "        for i in range(len(X_train)):\n",
    "            if (len(Xs) == batch_size):\n",
    "                yield (np.array(Xs), np.array(ys))\n",
    "                Xs = []\n",
    "                ys = []\n",
    "                \n",
    "            x, y = process_line({'path':X_train[i], 'steering':y_train[i]})\n",
    "            Xs.append(x)\n",
    "            ys.append(y)\n",
    "            \n",
    "            if flip:\n",
    "                if (len(Xs) == batch_size):\n",
    "                    yield (np.array(Xs), np.array(ys))\n",
    "                    Xs = []\n",
    "                    ys = []\n",
    "                    \n",
    "                x_flipped = np.fliplr(x)\n",
    "                y_filpped = -y\n",
    "\n",
    "                Xs.append(x_flipped)\n",
    "                ys.append(y_filpped)\n",
    "\n",
    "        yield (np.array(Xs), np.array(ys))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_rows = len(X_train)\n",
    "validation_rows = len(X_validation)\n",
    "    \n",
    "print('train records: ', train_rows)\n",
    "print('validation records: ', validation_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model = get_model()\n",
    "optimizer = RMSprop(lr=0.0001)\n",
    "model.compile(loss='mean_absolute_error', optimizer=optimizer)\n",
    "checkpoint = ModelCheckpoint('model1.h5', monitor='val_loss', verbose=1, save_best_only=True,\n",
    "                                 save_weights_only=False, mode='auto')\n",
    "history = model.fit_generator(\n",
    "    generate_arrays_from_file('train.csv',batch_size=100), \n",
    "    samples_per_epoch=train_rows*2, \n",
    "    nb_epoch=30, \n",
    "    validation_data=generate_arrays_from_file('validation.csv', batch_size=10, flip=True),\n",
    "    nb_val_samples=validation_rows*2, \n",
    "    callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
