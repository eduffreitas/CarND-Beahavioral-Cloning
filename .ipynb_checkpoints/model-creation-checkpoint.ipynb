{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CarND Behavioral Cloning Project\n",
    "\n",
    "This project is about training a neural network to drive a car on a simulator using data recorded from a humman driver.\n",
    "\n",
    "This notebook will be used to create the model to be used in driving the car on the simulator.\n",
    "\n",
    "The inputs come in three images right, central and left cameras.\n",
    "\n",
    "The first thing to do is to clean, then oganize the dataset, and save it to pickle file. for posterior use.\n",
    "\n",
    "The file driving_log.csv contains steering angles and the left, right and center images associated to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import csv\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickling done\n"
     ]
    }
   ],
   "source": [
    "path_to_replace = \"C:\\\\Users\\\\eduardo\\\\Documents\\\\SelfDrivingCar\\\\simulator-windows-64\\\\\"\n",
    "\n",
    "def ReplaceWrongPath(value):\n",
    "    return value.replace(path_to_replace, \"\").replace(\"\\\\\", \"/\").replace(\" data\", \"data\")\n",
    "\n",
    "center_imgs = []\n",
    "left_imgs = []\n",
    "right_imgs = []\n",
    "labels = []\n",
    "\n",
    "with open('./data/driving_log.csv', 'r') as csvfile:\n",
    "    csv_reader = csv.reader(csvfile)\n",
    "    for row in csv_reader:\n",
    "        \n",
    "        b,g,r = cv2.split(cv2.imread('./' + ReplaceWrongPath(row[0])))\n",
    "        center_imgs.append(np.array(cv2.merge([r,g,b])))\n",
    "        b,g,r = cv2.split(cv2.imread('./' + ReplaceWrongPath(row[1])))\n",
    "        left_imgs.append(np.array(cv2.merge([r,g,b])))\n",
    "        b,g,r = cv2.split(cv2.imread('./' + ReplaceWrongPath(row[2])))\n",
    "        right_imgs.append(np.array(cv2.merge([r,g,b])))\n",
    "        labels.append(row[3])\n",
    "        \n",
    "pickle.dump({\"features_center\": center_imgs, \"features_left\": left_imgs, \"features_right\": right_imgs, \"labels\": labels }, \n",
    "            open( \"driving_data.p\", \"wb\" ))\n",
    "\n",
    "print(\"pickling done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_file = 'driving_data.p'\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "\n",
    "X_train = train['features_center']\n",
    "y_train = train['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Visualizations will be shown in the notebook.\n",
    "%matplotlib inline\n",
    "\n",
    "rand_image_index = np.random.randint(0, len(X_train))\n",
    "#plt.figure(figsize=(1,1))\n",
    "plt.imshow(X_train[rand_image_index].squeeze(), cmap='gray')\n",
    "print(y_train[rand_image_index])\n",
    "print(rand_image_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Normalize(image_data):\n",
    "    a = 0.1\n",
    "    b = 0.9\n",
    "    img_min = 0\n",
    "    img_max = 255\n",
    "    return a + ( ( (image_data - img_min)*(b - a) )/( img_max - img_min ) )\n",
    "    \n",
    "X_train = Normalize(X_train) \n",
    "\n",
    "print(\"Min\", np.min(X_train))\n",
    "print(\"Max\", np.max(X_train))\n",
    "print(\"Std\", np.std(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Split train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_copy, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "#### VGG with drop out, batch normalition\n",
    "\n",
    "#### 20 EPOCHS Adam optimizer linear regression\n",
    "\n",
    "#### Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
