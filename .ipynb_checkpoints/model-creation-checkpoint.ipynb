{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CarND Behavioral Cloning Project\n",
    "\n",
    "This project is about training a neural network to drive a car on a simulator using data recorded from a humman driver.\n",
    "\n",
    "This notebook will be used to create the model to be used in driving the car on the simulator.\n",
    "\n",
    "The inputs come in three images right, central and left cameras.\n",
    "\n",
    "The first thing to do is to clean, then oganize the dataset, and save it to pickle file. for posterior use.\n",
    "\n",
    "The file driving_log.csv contains steering angles and the left, right and center images associated to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import csv\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os.path\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files exist\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if not(os.path.exists('train.csv') and os.path.exists('validation.csv')):\n",
    "\n",
    "    path_to_replace = \"C:\\\\Users\\\\eduardo\\\\Documents\\\\SelfDrivingCar\\\\beta-simulator-windows\\\\beta_simulator_windows\\\\CenterLaps\"\n",
    "\n",
    "    def ReplaceWrongPath(value):\n",
    "        return value.replace(path_to_replace, \"\").replace(\"\\\\\", \"/\").replace(\" \", \"\")\n",
    "\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    X_train_left = []\n",
    "    y_train_left = []\n",
    "    X_train_right = []\n",
    "    y_train_right = []\n",
    "\n",
    "    with open('./data/driving_log.csv', 'r') as csv_file_in:\n",
    "\n",
    "        csv_reader = csv.DictReader(csv_file_in)\n",
    "\n",
    "        for row in csv_reader:\n",
    "            steering = float(row['steering'])\n",
    "\n",
    "            #center image\n",
    "            path = './data/' + ReplaceWrongPath(row['center'].strip())        \n",
    "            X_train.append(path)\n",
    "            y_train.append(steering)\n",
    "            continue\n",
    "            \n",
    "            if steering == 0:\n",
    "                continue\n",
    "                \n",
    "            if steering < 0:\n",
    "                #left image\n",
    "                path = './data/' + ReplaceWrongPath(row['left'].strip())\n",
    "                steering_left = steering + 0.2\n",
    "\n",
    "                X_train_left.append(path)\n",
    "                y_train_left.append(steering_left)\n",
    "\n",
    "                #right image\n",
    "                path = './data/' + ReplaceWrongPath(row['right'].strip())\n",
    "                steering_right = steering - 0.2\n",
    "                steering_right = steering_right if steering_right > -1 else -1\n",
    "\n",
    "                X_train_right.append(path)\n",
    "                y_train_right.append(steering_right)\n",
    "            else:\n",
    "                #left image\n",
    "                path = './data/' + ReplaceWrongPath(row['left'].strip())\n",
    "                steering_left = steering + 0.2\n",
    "                steering_left = steering_left if steering_left < 1 else 1\n",
    "\n",
    "                X_train_left.append(path)\n",
    "                y_train_left.append(steering_left)\n",
    "\n",
    "                #right image\n",
    "                path = './data/' + ReplaceWrongPath(row['right'].strip())\n",
    "                steering_right = steering - 0.2\n",
    "\n",
    "                X_train_right.append(path)\n",
    "                y_train_right.append(steering_right)\n",
    "\n",
    "    X_train, y_train = shuffle(X_train, y_train)\n",
    "    X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "    [X_train.append(item) for item in X_train_left]\n",
    "    [X_train.append(item) for item in X_train_right]\n",
    "    [y_train.append(item) for item in y_train_left]\n",
    "    [y_train.append(item) for item in y_train_right]\n",
    "    \n",
    "    X_train, y_train = shuffle(X_train, y_train)\n",
    "\n",
    "    with open('train.csv', 'w') as csv_file_train:\n",
    "\n",
    "        fieldnames = ['path','steering']\n",
    "        writer = csv.DictWriter(csv_file_train, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for i in range(len(X_train)):\n",
    "            path, steering = X_train[i], y_train[i]\n",
    "            writer.writerow({'path': path, 'steering': steering})\n",
    "\n",
    "    with open('validation.csv', 'w') as csv_file_train:\n",
    "\n",
    "        fieldnames = ['path','steering']\n",
    "        writer = csv.DictWriter(csv_file_train, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for i in range(len(X_validation)):\n",
    "            path, steering = X_validation[i], y_validation[i]\n",
    "            writer.writerow({'path': path, 'steering': steering})\n",
    "\n",
    "    print(\"processing done\")\n",
    "else:\n",
    "    print(\"files exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading done\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "def turn_linear_to_logistic(steering, n_classes):\n",
    "    interval = 2/n_classes\n",
    "    classes = []\n",
    "    lower_bound = -1\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        upper_bound = lower_bound + interval if i < (n_classes-1) else 1\n",
    "        classes.append(1 if steering > lower_bound and steering < upper_bound else 0)\n",
    "        lower_bound += interval\n",
    "        \n",
    "    return np.array(classes)\n",
    "    \n",
    "    \n",
    "break_classes = 21\n",
    "X_train_left = None\n",
    "y_train_left = None\n",
    "X_train_right = None\n",
    "y_train_right = None\n",
    "\n",
    "X_train= []\n",
    "y_train = []\n",
    "\n",
    "with open('train.csv', 'r') as csv_file_train:\n",
    "    \n",
    "    csv_reader = csv.DictReader(csv_file_train)\n",
    "\n",
    "    for row in csv_reader:\n",
    "        X_train.append(row['path'])\n",
    "        y_train.append(float(row['steering']))\n",
    "        \n",
    "\n",
    "X_validation = []\n",
    "y_validation = []\n",
    "\n",
    "with open('validation.csv', 'r') as csv_file_val:\n",
    "    \n",
    "    csv_reader = csv.DictReader(csv_file_val)\n",
    "\n",
    "    for row in csv_reader:\n",
    "        X_validation.append(row['path'])\n",
    "        y_validation.append(float(row['steering']))\n",
    "        \n",
    "print('Loading done')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi0AAAGHCAYAAAB8hmJnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f09fd8cd668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y_train, bins=70)\n",
    "plt.title(\"Training data\")\n",
    "plt.xlabel(\"Angles\")\n",
    "plt.ylabel(\"# of examples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalize(image_data):\n",
    "    a = -0.5\n",
    "    b = 0.5\n",
    "    img_min = 0\n",
    "    img_max = 255\n",
    "    return a + ( ( (image_data - img_min)*(b - a) )/( img_max - img_min ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "#### VGG with drop out, batch normalition\n",
    "\n",
    "#### 20 EPOCHS Adam optimizer linear regression\n",
    "\n",
    "#### Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Lambda, Dense, Activation, Flatten, Input, Dropout, Convolution2D, MaxPooling2D, Cropping2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "input_tensor = Input(shape=(160, 320, 3))\n",
    "croped_input_img = Cropping2D(cropping=((60, 20), (0, 0)))(input_tensor)\n",
    "croped_input_img = Lambda(lambda x: (x / 255.0) - 0.5)(croped_input_img)\n",
    "base_model = VGG16(input_tensor=croped_input_img, weights='imagenet', include_top=False)\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "top_model = base_model.output\n",
    "top_model = Flatten()(top_model)\n",
    "\n",
    "top_model = Dense(1024)(top_model)\n",
    "top_model = BatchNormalization()(top_model)\n",
    "top_model = Activation('relu')(top_model)\n",
    "top_model = Dropout(0.5)(top_model)\n",
    "\n",
    "top_model = Dense(100)(top_model)\n",
    "top_model = BatchNormalization()(top_model)\n",
    "top_model = Activation('relu')(top_model)\n",
    "top_model = Dropout(0.5)(top_model)\n",
    "\n",
    "top_model = Dense(50)(top_model)\n",
    "top_model = BatchNormalization()(top_model)\n",
    "top_model = Activation('relu')(top_model)\n",
    "top_model = Dropout(0.5)(top_model)\n",
    "\n",
    "top_model = Dense(10)(top_model)\n",
    "top_model = BatchNormalization()(top_model)\n",
    "top_model = Activation('relu')(top_model)\n",
    "top_model = Dropout(0.5)(top_model)\n",
    "\n",
    "predictions = Dense(1)(top_model)\n",
    "\n",
    "model = Model(input=input_tensor, output=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from keras.layers import merge, ZeroPadding2D, Cropping2D, Convolution2D, MaxPooling2D, Input, Lambda, Flatten, Dense, Activation, Dropout\n",
    "from keras.models import Model\n",
    "\n",
    "def AddConvLayer(features, kernel, input_layer):\n",
    "    complex_layer = ZeroPadding2D((1,1))(input_layer)\n",
    "    complex_layer = Convolution2D(features, kernel[0], kernel[1], activation='relu')(complex_layer)\n",
    "    complex_layer = Dropout(0.5)(complex_layer)\n",
    "    complex_layer = ZeroPadding2D((1,1))(complex_layer)\n",
    "    complex_layer = Convolution2D(features, kernel[0], kernel[1], activation='relu')(complex_layer)\n",
    "    complex_layer = Dropout(0.5)(complex_layer)\n",
    "    complex_layer = MaxPooling2D((2,2), strides=(2,2))(complex_layer)\n",
    "    return complex_layer\n",
    "\n",
    "def AddDenseLayer(neurons, depth, layer):\n",
    "    for i in range(depth):\n",
    "        layer = Dense(neurons, activation='relu')(layer)\n",
    "        layer = Dropout(0.5)(layer)\n",
    "    return layer\n",
    "\n",
    "\n",
    "input_img = Input(shape=(160, 320, 3))\n",
    "croped_input_img = Cropping2D(cropping=((60, 20), (0, 0)))(input_img)\n",
    "croped_input_img = Lambda(lambda x: (x / 255.0) - 0.5)(croped_input_img)\n",
    "\n",
    "#tower_1 = AddConvLayer(6, (3, 2), croped_input_img)\n",
    "#tower_1 = AddConvLayer(6, (3, 2), tower_1)\n",
    "#tower_1 = AddConvLayer(6, (3, 2), tower_1)\n",
    "#tower_1 = AddConvLayer(16, (3, 2), tower_1)\n",
    "#tower_1 = AddConvLayer(16, (3, 2), tower_1)\n",
    "#tower_1 = AddConvLayer(16, (3, 2), tower_1)\n",
    "#tower_1 = Flatten()(tower_1)\n",
    "#tower_1 = AddDenseLayer(1024, 2, tower_1)\n",
    "\n",
    "tower_2_1 = AddConvLayer(32, (5, 5), croped_input_img)\n",
    "\n",
    "tower_2_1_1 = AddConvLayer(64, (5, 5), tower_2_1)\n",
    "\n",
    "tower_2_1_1_1 = AddConvLayer(128, (5, 5), tower_2_1_1)\n",
    "tower_2_1_1_2 = AddConvLayer(256, (3, 3), tower_2_1_1_1)\n",
    "tower_2_1_1_3 = AddConvLayer(512, (3, 3), tower_2_1_1_2)\n",
    "tower_2_1_1_4 = Flatten()(tower_2_1_1_3)\n",
    "\n",
    "tower_2_1_2 = AddConvLayer(128, (5, 5), tower_2_1_1)\n",
    "tower_2_1_3 = AddConvLayer(256, (3, 3), tower_2_1_2)\n",
    "tower_2_1_4 = AddConvLayer(512, (3, 3), tower_2_1_3)\n",
    "tower_2_1_5 = Flatten()(tower_2_1_4)\n",
    "\n",
    "tower_2_2 = AddConvLayer(64, (5, 5), tower_2_1)\n",
    "tower_2_3 = AddConvLayer(128, (5, 5), tower_2_2)\n",
    "tower_2_4 = AddConvLayer(256, (3, 3), tower_2_3)\n",
    "tower_2_5 = AddConvLayer(512, (3, 3), tower_2_4)\n",
    "tower_2_6 = Flatten()(tower_2_5)\n",
    "\n",
    "tower_2_6 = merge([tower_2_6, tower_2_1_5, tower_2_1_1_4], mode='concat', concat_axis=1)\n",
    "\n",
    "output = AddDenseLayer(1024, 1, tower_2_6)\n",
    "output = AddDenseLayer(84, 1, output)\n",
    "output = AddDenseLayer(50, 1, output)\n",
    "\n",
    "#tower_3 = AddConvLayer(6, (5, 2), croped_input_img)\n",
    "#tower_3 = AddConvLayer(6, (5, 2), tower_3)\n",
    "#tower_3 = AddConvLayer(16, (5, 2), tower_3)\n",
    "#tower_3 = AddConvLayer(16, (5, 2), tower_3)\n",
    "#tower_3 = Flatten()(tower_3)\n",
    "#tower_3 = AddDenseLayer(1024, 2, tower_3)\n",
    "\n",
    "#output = merge([tower_1, tower_2, tower_3], mode='concat', concat_axis=1)\n",
    "\n",
    "predictions = Dense(56, activation='relu')(output)\n",
    "predictions = Dropout(0.5)(predictions)\n",
    "predictions = Dense(1, activation='tanh')(predictions)\n",
    "\n",
    "model = Model(input=input_img, output=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_line(row):\n",
    "    b,g,r = cv2.split(cv2.imread(row['path']))\n",
    "    img = np.array(cv2.merge([r,g,b]))\n",
    "        \n",
    "    #steering = float(row['steering'])            \n",
    "    steering = row['steering']\n",
    "    return [img, steering]\n",
    "\n",
    "def generate_arrays_from_file(path, batch_size = 20, flip=True):\n",
    "    while 1:\n",
    "        global X_train\n",
    "        global y_train\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        Xs = []\n",
    "        ys = []        \n",
    "        for i in range(len(X_train)):\n",
    "            if (len(Xs) == batch_size):\n",
    "                yield (np.array(Xs), np.array(ys))\n",
    "                Xs = []\n",
    "                ys = []\n",
    "                \n",
    "            x, y = process_line({'path':X_train[i], 'steering':y_train[i]})\n",
    "            Xs.append(x)\n",
    "            ys.append(y)\n",
    "            \n",
    "            if flip:\n",
    "                if (len(Xs) == batch_size):\n",
    "                    yield (np.array(Xs), np.array(ys))\n",
    "                    Xs = []\n",
    "                    ys = []\n",
    "                    \n",
    "                x_flipped = np.fliplr(x)\n",
    "                y_filpped = -y\n",
    "\n",
    "                Xs.append(x_flipped)\n",
    "                ys.append(y_filpped)\n",
    "\n",
    "        yield (np.array(Xs), np.array(ys))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train records:  5651\n",
      "validation records:  1413\n"
     ]
    }
   ],
   "source": [
    "train_rows = len(X_train)\n",
    "validation_rows = len(X_validation)\n",
    "    \n",
    "print('train records: ', train_rows)\n",
    "print('validation records: ', validation_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_1 False\n",
      "1 cropping2d_1 False\n",
      "2 lambda_1 False\n",
      "3 block1_conv1 False\n",
      "4 block1_conv2 False\n",
      "5 block1_pool False\n",
      "6 block2_conv1 False\n",
      "7 block2_conv2 False\n",
      "8 block2_pool False\n",
      "9 block3_conv1 False\n",
      "10 block3_conv2 False\n",
      "11 block3_conv3 False\n",
      "12 block3_pool False\n",
      "13 block4_conv1 False\n",
      "14 block4_conv2 False\n",
      "15 block4_conv3 False\n",
      "16 block4_pool False\n",
      "17 block5_conv1 False\n",
      "18 block5_conv2 False\n",
      "19 block5_conv3 False\n",
      "20 block5_pool False\n",
      "21 flatten_1 True\n",
      "22 dense_1 True\n",
      "23 batchnormalization_1 True\n",
      "24 activation_1 True\n",
      "25 dropout_1 True\n",
      "26 dense_2 True\n",
      "27 batchnormalization_2 True\n",
      "28 activation_2 True\n",
      "29 dropout_2 True\n",
      "30 dense_3 True\n",
      "31 batchnormalization_3 True\n",
      "32 activation_3 True\n",
      "33 dropout_3 True\n",
      "34 dense_4 True\n",
      "35 batchnormalization_4 True\n",
      "36 activation_4 True\n",
      "37 dropout_4 True\n",
      "38 dense_5 True\n"
     ]
    }
   ],
   "source": [
    "for i, Layer in enumerate(model.layers):\n",
    "    if i > 2 and i < 8:\n",
    "        print(i, Layer.name, Layer.trainable)\n",
    "    else:\n",
    "        print(i, Layer.name, Layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "11302/11302 [==============================] - 322s - loss: 0.2751 - val_loss: 0.0438\n",
      "Epoch 2/2\n",
      "11302/11302 [==============================] - 316s - loss: 0.0520 - val_loss: 0.0348\n",
      "0 input_1 False\n",
      "1 cropping2d_1 False\n",
      "2 lambda_1 False\n",
      "3 block1_conv1 True\n",
      "4 block1_conv2 True\n",
      "5 block1_pool True\n",
      "6 block2_conv1 True\n",
      "7 block2_conv2 True\n",
      "8 block2_pool True\n",
      "9 block3_conv1 False\n",
      "10 block3_conv2 False\n",
      "11 block3_conv3 False\n",
      "12 block3_pool False\n",
      "13 block4_conv1 False\n",
      "14 block4_conv2 False\n",
      "15 block4_conv3 False\n",
      "16 block4_pool False\n",
      "17 block5_conv1 False\n",
      "18 block5_conv2 False\n",
      "19 block5_conv3 False\n",
      "20 block5_pool False\n",
      "21 flatten_1 False\n",
      "22 dense_1 False\n",
      "23 batchnormalization_1 False\n",
      "24 activation_1 False\n",
      "25 dropout_1 False\n",
      "26 dense_2 False\n",
      "27 batchnormalization_2 False\n",
      "28 activation_2 False\n",
      "29 dropout_2 False\n",
      "30 dense_3 False\n",
      "31 batchnormalization_3 False\n",
      "32 activation_3 False\n",
      "33 dropout_3 False\n",
      "34 dense_4 False\n",
      "35 batchnormalization_4 False\n",
      "36 activation_4 False\n",
      "37 dropout_4 False\n",
      "38 dense_5 False\n",
      "Epoch 1/1\n",
      "11302/11302 [==============================] - 673s - loss: 0.0488 - val_loss: 0.0421\n",
      "0 input_1 False\n",
      "1 cropping2d_1 False\n",
      "2 lambda_1 False\n",
      "3 block1_conv1 False\n",
      "4 block1_conv2 False\n",
      "5 block1_pool False\n",
      "6 block2_conv1 False\n",
      "7 block2_conv2 False\n",
      "8 block2_pool False\n",
      "9 block3_conv1 True\n",
      "10 block3_conv2 True\n",
      "11 block3_conv3 True\n",
      "12 block3_pool True\n",
      "13 block4_conv1 True\n",
      "14 block4_conv2 True\n",
      "15 block4_conv3 True\n",
      "16 block4_pool True\n",
      "17 block5_conv1 False\n",
      "18 block5_conv2 False\n",
      "19 block5_conv3 False\n",
      "20 block5_pool False\n",
      "21 flatten_1 False\n",
      "22 dense_1 False\n",
      "23 batchnormalization_1 False\n",
      "24 activation_1 False\n",
      "25 dropout_1 False\n",
      "26 dense_2 False\n",
      "27 batchnormalization_2 False\n",
      "28 activation_2 False\n",
      "29 dropout_2 False\n",
      "30 dense_3 False\n",
      "31 batchnormalization_3 False\n",
      "32 activation_3 False\n",
      "33 dropout_3 False\n",
      "34 dense_4 False\n",
      "35 batchnormalization_4 False\n",
      "36 activation_4 False\n",
      "37 dropout_4 False\n",
      "38 dense_5 False\n",
      "Epoch 1/1\n",
      "11302/11302 [==============================] - 613s - loss: 0.0509 - val_loss: 0.1248\n",
      "0 input_1 False\n",
      "1 cropping2d_1 False\n",
      "2 lambda_1 False\n",
      "3 block1_conv1 False\n",
      "4 block1_conv2 False\n",
      "5 block1_pool False\n",
      "6 block2_conv1 False\n",
      "7 block2_conv2 False\n",
      "8 block2_pool False\n",
      "9 block3_conv1 False\n",
      "10 block3_conv2 False\n",
      "11 block3_conv3 False\n",
      "12 block3_pool False\n",
      "13 block4_conv1 False\n",
      "14 block4_conv2 False\n",
      "15 block4_conv3 False\n",
      "16 block4_pool False\n",
      "17 block5_conv1 True\n",
      "18 block5_conv2 True\n",
      "19 block5_conv3 True\n",
      "20 block5_pool True\n",
      "21 flatten_1 False\n",
      "22 dense_1 False\n",
      "23 batchnormalization_1 False\n",
      "24 activation_1 False\n",
      "25 dropout_1 False\n",
      "26 dense_2 False\n",
      "27 batchnormalization_2 False\n",
      "28 activation_2 False\n",
      "29 dropout_2 False\n",
      "30 dense_3 False\n",
      "31 batchnormalization_3 False\n",
      "32 activation_3 False\n",
      "33 dropout_3 False\n",
      "34 dense_4 False\n",
      "35 batchnormalization_4 False\n",
      "36 activation_4 False\n",
      "37 dropout_4 False\n",
      "38 dense_5 False\n",
      "Epoch 1/1\n",
      "11302/11302 [==============================] - 334s - loss: 0.0502 - val_loss: 0.0613\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import RMSprop, Adam\n",
    "\n",
    "optimizer = RMSprop(lr=0.001)\n",
    "model.compile(loss='mean_absolute_error', optimizer=optimizer)\n",
    "history = model.fit_generator(generate_arrays_from_file('train.csv',batch_size=10), samples_per_epoch=train_rows*2, nb_epoch=2, validation_data=generate_arrays_from_file('validation.csv', batch_size=10, flip=False),nb_val_samples=validation_rows)\n",
    "\n",
    "for i, Layer in enumerate(model.layers):\n",
    "    if i > 2 and i < 9:\n",
    "        Layer.trainable = True\n",
    "        print(i, Layer.name, True)\n",
    "    else:\n",
    "        Layer.trainable = False\n",
    "        print(i, Layer.name, False)\n",
    "\n",
    "optimizer = Adam(lr=0.0001, decay=0.0001)\n",
    "model.compile(loss='mean_absolute_error', optimizer=optimizer)\n",
    "history = model.fit_generator(generate_arrays_from_file('train.csv',batch_size=20), samples_per_epoch=train_rows*2, nb_epoch=1, validation_data=generate_arrays_from_file('validation.csv', batch_size=10, flip=False),nb_val_samples=validation_rows)\n",
    "\n",
    "for i, Layer in enumerate(model.layers):\n",
    "    if i > 8 and i < 17:\n",
    "        Layer.trainable = True\n",
    "        print(i, Layer.name, True)\n",
    "    else:\n",
    "        Layer.trainable = False\n",
    "        print(i, Layer.name, False)\n",
    "\n",
    "optimizer = Adam(lr=0.0001, decay=0.0001)\n",
    "model.compile(loss='mean_absolute_error', optimizer=optimizer)\n",
    "history = model.fit_generator(generate_arrays_from_file('train.csv',batch_size=20), samples_per_epoch=train_rows*2, nb_epoch=1, validation_data=generate_arrays_from_file('validation.csv', batch_size=10, flip=False),nb_val_samples=validation_rows)\n",
    "\n",
    "for i, Layer in enumerate(model.layers):\n",
    "    if i > 16 and i < 21:\n",
    "        Layer.trainable = True\n",
    "        print(i, Layer.name, True)\n",
    "    else:\n",
    "        Layer.trainable = False\n",
    "        print(i, Layer.name, False)\n",
    "\n",
    "optimizer = Adam(lr=0.0001, decay=0.0001)\n",
    "model.compile(loss='mean_absolute_error', optimizer=optimizer)\n",
    "history = model.fit_generator(generate_arrays_from_file('train.csv',batch_size=20), samples_per_epoch=train_rows*2, nb_epoch=1, validation_data=generate_arrays_from_file('validation.csv', batch_size=10, flip=False),nb_val_samples=validation_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truth:  -0.1698113 Pred:  -0.00654884 Error:  -0.163262459447 Loss:  0.163262459447\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0826333425929\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.055756970308\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0493942566656\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0455766284802\n",
      "Truth:  0.06603774 Pred:  0.00200423 Error:  0.0640335142617 Loss:  0.0486527761104\n",
      "Truth:  -0.1509434 Pred:  0.00200423 Error:  -0.152947625738 Loss:  0.063552040343\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0593962997674\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0530194026531\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0507480739616\n",
      "Truth:  -0.06603774 Pred:  0.00200423 Error:  -0.0680419657383 Loss:  0.0523202459413\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0496995709244\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0460306982178\n",
      "Truth:  0.1981132 Pred:  0.00200423 Error:  0.196108974262 Loss:  0.056750575078\n",
      "Truth:  0.1509434 Pred:  0.00200423 Error:  0.148939174262 Loss:  0.0628964816903\n",
      "Truth:  -0.09433963 Pred:  -0.0126913 Error:  -0.0816483436075 Loss:  0.0640684730601\n",
      "Truth:  0.2264151 Pred:  0.00200423 Error:  0.224410874262 Loss:  0.0735003790131\n",
      "Truth:  0.1226415 Pred:  0.00200423 Error:  0.120637274262 Loss:  0.0761190954158\n",
      "Truth:  -0.1698113 Pred:  -0.0109844 Error:  -0.158826852215 Loss:  0.0804721352474\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0765487397719\n",
      "Truth:  0.2075472 Pred:  0.00200423 Error:  0.205542974262 Loss:  0.0826913223667\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0790237270654\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0756750530947\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0726054352881\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0697813869061\n",
      "Truth:  -0.009433962 Pred:  0.00200423 Error:  -0.0114381877383 Loss:  0.0675374177074\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0658090743011\n",
      "Truth:  0.04716982 Pred:  0.00141552 Error:  0.0457543004905 Loss:  0.0650928323793\n",
      "Truth:  -0.09433963 Pred:  0.00200423 Error:  -0.0963438557383 Loss:  0.0661704538745\n",
      "Truth:  -0.05660377 Pred:  0.00200423 Error:  -0.0586079957383 Loss:  0.0659183719366\n",
      "Truth:  -0.06603774 Pred:  0.00200423 Error:  -0.0680419657383 Loss:  0.0659868749624\n",
      "Truth:  0.08490566 Pred:  0.00200423 Error:  0.0829014342617 Loss:  0.0665154549405\n",
      "Truth:  0.0 Pred:  -0.0161258 Error:  0.0161257609725 Loss:  0.0649884945173\n",
      "Truth:  -0.0754717 Pred:  0.00159205 Error:  -0.0770637485603 Loss:  0.065343649048\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0640730346677\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0623489010863\n",
      "Truth:  -0.1037736 Pred:  -0.000522836 Error:  -0.103250763656 Loss:  0.0634543568314\n",
      "Truth:  -0.254717 Pred:  -0.0339586 Error:  -0.220758449457 Loss:  0.0675939382163\n",
      "Truth:  -0.05660377 Pred:  0.00200423 Error:  -0.0586079957383 Loss:  0.0673635294348\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0657295468424\n",
      "Truth:  0.0 Pred:  -0.0341793 Error:  0.0341793000698 Loss:  0.0649600286285\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0634610809406\n",
      "Truth:  0.3113208 Pred:  0.00200423 Error:  0.309316574262 Loss:  0.0691786505527\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0676519590797\n",
      "Truth:  -0.09433963 Pred:  -0.00762651 Error:  -0.0867131188434 Loss:  0.0680755404077\n",
      "Truth:  -0.03773585 Pred:  0.00200423 Error:  -0.0397400757383 Loss:  0.0674595520454\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0660668855282\n",
      "Truth:  -0.05660377 Pred:  0.00200423 Error:  -0.0586079957383 Loss:  0.0659114919909\n",
      "Truth:  -0.254717 Pred:  -0.0317802 Error:  -0.222936768256 Loss:  0.0691160894657\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0677738521912\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0664842516725\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0657885182891\n",
      "Truth:  -0.1509434 Pred:  -0.00450188 Error:  -0.146441521716 Loss:  0.0673102730708\n",
      "Truth:  -0.03773585 Pred:  0.00200423 Error:  -0.0397400757383 Loss:  0.0667997138609\n",
      "Truth:  0.009433962 Pred:  0.00200423 Error:  0.00742973626165 Loss:  0.0657202597227\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0649194005444\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0638156255478\n",
      "Truth:  0.0 Pred:  -5.84803e-05 Error:  5.84803055972e-05 Loss:  0.062716364423\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0616873451232\n",
      "Truth:  0.02830189 Pred:  0.00200423 Error:  0.0262976642617 Loss:  0.0610975171089\n",
      "Truth:  -0.1509434 Pred:  0.00058275 Error:  -0.151526149723 Loss:  0.0625799537091\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0616029258386\n",
      "Truth:  -0.03773585 Pred:  0.00200423 Error:  -0.0397400757383 Loss:  0.0612558964719\n",
      "Truth:  -0.0754717 Pred:  0.00200423 Error:  -0.0774759257383 Loss:  0.0615093344292\n",
      "Truth:  -0.09433963 Pred:  -0.0144076 Error:  -0.079931986883 Loss:  0.0617927598516\n",
      "Truth:  0.02830189 Pred:  0.00200423 Error:  0.0262976642617 Loss:  0.061254955373\n",
      "Truth:  -0.009433962 Pred:  0.00200423 Error:  -0.0114381877383 Loss:  0.0605114215277\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0596510215896\n",
      "Truth:  -0.06603774 Pred:  0.000883113 Error:  -0.0669208531272 Loss:  0.0597563814669\n",
      "Truth:  -0.05660377 Pred:  -0.0320776 Error:  -0.0245261259797 Loss:  0.0592530921028\n",
      "Truth:  -0.0754717 Pred:  0.00164044 Error:  -0.0771121356575 Loss:  0.0595046279276\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.059099093036\n",
      "Truth:  -0.05660377 Pred:  0.00200423 Error:  -0.0586079957383 Loss:  0.0590923656758\n",
      "Truth:  -0.09433963 Pred:  0.000195292 Error:  -0.0945349219411 Loss:  0.0595713191388\n",
      "Truth:  -0.1037736 Pred:  -0.000307983 Error:  -0.103465617445 Loss:  0.0601565764496\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0596396760455\n",
      "Truth:  -0.009433962 Pred:  0.00200423 Error:  -0.0114381877383 Loss:  0.0590136826909\n",
      "Truth:  -0.05660377 Pred:  0.00159685 Error:  -0.0582006239519 Loss:  0.0590032588609\n",
      "Truth:  0.245283 Pred:  0.00200423 Error:  0.243278774262 Loss:  0.0613358603216\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0608300638893\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0601038189739\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0593952873491\n",
      "Truth:  -0.06603774 Pred:  -0.00277267 Error:  -0.0632650725548 Loss:  0.0594419112672\n",
      "Truth:  -0.03773585 Pred:  0.00200423 Error:  -0.0397400757383 Loss:  0.0592073656062\n",
      "Truth:  -0.03773585 Pred:  0.00200423 Error:  -0.0397400757383 Loss:  0.0589783386665\n",
      "Truth:  -0.08490566 Pred:  0.00200423 Error:  -0.0869098857383 Loss:  0.0593031240976\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0588613887142\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0582152845894\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0575836996585\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0571757935039\n",
      "Truth:  -0.04716982 Pred:  0.00200423 Error:  -0.0491740457383 Loss:  0.0570878622097\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0567967562698\n",
      "Truth:  -0.1132075 Pred:  -0.0298624 Error:  -0.0833450551522 Loss:  0.0570822218492\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0564962857204\n",
      "Truth:  -0.02830189 Pred:  -0.00956061 Error:  -0.0187412835576 Loss:  0.056098864645\n",
      "Truth:  0.3396226 Pred:  0.00200423 Error:  0.337618374262 Loss:  0.0590313595368\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.058443450941\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0578675404797\n",
      "Truth:  0.0 Pred:  0.00111263 Error:  -0.00111262674909 Loss:  0.0572942585228\n",
      "Truth:  -0.03773585 Pred:  0.00200423 Error:  -0.0397400757383 Loss:  0.057118716695\n",
      "Truth:  -0.09433963 Pred:  0.00200423 Error:  -0.0963438557383 Loss:  0.0575070844083\n",
      "Truth:  -0.1320755 Pred:  0.00200423 Error:  -0.134079725738 Loss:  0.0582577965782\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.057711645405\n",
      "Truth:  -0.03773585 Pred:  0.00200423 Error:  -0.0397400757383 Loss:  0.0575388418505\n",
      "Truth:  -0.009433962 Pred:  0.00200423 Error:  -0.0114381877383 Loss:  0.0570997880018\n",
      "Truth:  -0.009433962 Pred:  0.00200423 Error:  -0.0114381877383 Loss:  0.056669018188\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0561581322773\n",
      "Truth:  -0.06603774 Pred:  0.00200423 Error:  -0.0680419657383 Loss:  0.0562681677723\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0559434336252\n",
      "Truth:  -0.254717 Pred:  -0.0496841 Error:  -0.205032896422 Loss:  0.0572987923779\n",
      "Truth:  -0.0754717 Pred:  0.0015189 Error:  -0.0769906044643 Loss:  0.0574761960904\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0569809106408\n",
      "Truth:  -0.06603774 Pred:  0.00200423 Error:  -0.0680419657383 Loss:  0.0570787960841\n",
      "Truth:  0.1132075 Pred:  0.00200423 Error:  0.111203274262 Loss:  0.0575535722085\n",
      "Truth:  0.0 Pred:  -0.00508331 Error:  0.00508330902085 Loss:  0.0570973090503\n",
      "Truth:  -0.009433962 Pred:  0.00200423 Error:  -0.0114381877383 Loss:  0.0567036959356\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0563974433698\n",
      "Truth:  -0.03773585 Pred:  0.00041753 Error:  -0.0381533798382 Loss:  0.0562428326619\n",
      "Truth:  -0.05660377 Pred:  -0.000404109 Error:  -0.0561996609858 Loss:  0.0562424698747\n",
      "Truth:  -0.08490566 Pred:  0.00200423 Error:  -0.0869098857383 Loss:  0.0564980316736\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.056047669641\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0557593456746\n",
      "Truth:  -0.1226415 Pred:  0.00200423 Error:  -0.124645725738 Loss:  0.0563193975451\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.055881371966\n",
      "Truth:  -0.04716982 Pred:  0.00200423 Error:  -0.0491740457383 Loss:  0.0558277133562\n",
      "Truth:  -0.05660377 Pred:  0.00200423 Error:  -0.0586079957383 Loss:  0.0558497790893\n",
      "Truth:  -0.03773585 Pred:  0.00200423 Error:  -0.0397400757383 Loss:  0.0557229310315\n",
      "Truth:  -0.2735849 Pred:  -0.0258137 Error:  -0.247771245935 Loss:  0.0572233084916\n",
      "Truth:  0.1792453 Pred:  0.00200423 Error:  0.177241074262 Loss:  0.0581536787689\n",
      "Truth:  -0.1320755 Pred:  0.00174362 Error:  -0.133819122823 Loss:  0.0587357206463\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0583026557996\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0580905607992\n",
      "Truth:  -0.08490566 Pred:  0.00200423 Error:  -0.0869098857383 Loss:  0.0583072474529\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.057887075649\n",
      "Truth:  0.04716982 Pred:  0.00200423 Error:  0.0451655942617 Loss:  0.0577928424536\n",
      "Truth:  0.1603774 Pred:  0.00200423 Error:  0.158373174262 Loss:  0.0585324037169\n",
      "Truth:  -0.05660377 Pred:  0.000619337 Error:  -0.0572231069627 Loss:  0.0585228468062\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0581132915811\n",
      "Truth:  -0.06603774 Pred:  0.00200423 Error:  -0.0680419657383 Loss:  0.0581847208916\n",
      "Truth:  0.1509434 Pred:  0.00200423 Error:  0.148939174262 Loss:  0.0588329669871\n",
      "Truth:  -0.2169811 Pred:  -0.0463405 Error:  -0.170640593476 Loss:  0.0596259288771\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0592201422353\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0590179462458\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0586220176311\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0583616736871\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0579756637696\n",
      "Truth:  -0.0754717 Pred:  0.00200423 Error:  -0.0774759257383 Loss:  0.0581083186129\n",
      "Truth:  -0.04716982 Pred:  0.00200423 Error:  -0.0491740457383 Loss:  0.0580479519043\n",
      "Truth:  -0.05660377 Pred:  0.00200423 Error:  -0.0586079957383 Loss:  0.0580517105878\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0578667399554\n",
      "Truth:  -0.1603774 Pred:  -0.0180505 Error:  -0.14232690819 Loss:  0.0584260788179\n",
      "Truth:  -0.06603774 Pred:  0.00200423 Error:  -0.0680419657383 Loss:  0.0584893412319\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.058243477209\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0578782872644\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0575178094481\n",
      "Truth:  -0.08490566 Pred:  0.00200423 Error:  -0.0869098857383 Loss:  0.0577062201936\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0574716082544\n",
      "Truth:  -0.1698113 Pred:  -0.0101832 Error:  -0.15962810721 Loss:  0.0581181683743\n",
      "Truth:  -0.04716982 Pred:  0.00200423 Error:  -0.0491740457383 Loss:  0.0580619160307\n",
      "Truth:  -0.05660377 Pred:  -0.000643446 Error:  -0.0559603243555 Loss:  0.0580487810827\n",
      "Truth:  -0.1226415 Pred:  -0.00588325 Error:  -0.11675825334 Loss:  0.0584134361899\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0582399342118\n",
      "Truth:  0.0 Pred:  -0.0272867 Error:  0.0272866860032 Loss:  0.0580500369836\n",
      "Truth:  -0.0754717 Pred:  0.00200423 Error:  -0.0774759257383 Loss:  0.0581684875247\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0578280980594\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0576623029852\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0573290210855\n",
      "Truth:  -0.04716982 Pred:  0.00200423 Error:  -0.0491740457383 Loss:  0.0572804795655\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0569534011405\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.056630170697\n",
      "Truth:  -0.06603774 Pred:  -0.0110039 Error:  -0.0550338480626 Loss:  0.0566208354769\n",
      "Truth:  -0.06603774 Pred:  0.00198306 Error:  -0.0680208005015 Loss:  0.0566871143433\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.056371028282\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0560585753938\n",
      "Truth:  -0.04716982 Pred:  0.00200423 Error:  -0.0491740457383 Loss:  0.0560192352244\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0557123317614\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0554088961341\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.055267869278\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0549703070236\n",
      "Truth:  -0.009433962 Pred:  0.00200423 Error:  -0.0114381877383 Loss:  0.0547284619164\n",
      "Truth:  0.0 Pred:  0.000341601 Error:  -0.000341600505635 Loss:  0.0544279820191\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0542954442923\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0540096999286\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.053727061482\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0536004617752\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0533230626568\n",
      "Truth:  -0.05660377 Pred:  0.00200423 Error:  -0.0586079957383 Loss:  0.053351324331\n",
      "Truth:  -0.1320755 Pred:  -0.000580851 Error:  -0.131494648779 Loss:  0.0537669803121\n",
      "Truth:  -0.06603774 Pred:  0.00200423 Error:  -0.0680419657383 Loss:  0.0538425093355\n",
      "Truth:  0.02830189 Pred:  0.00200423 Error:  0.0262976642617 Loss:  0.0536975364667\n",
      "Truth:  -0.04716982 Pred:  0.00200423 Error:  -0.0491740457383 Loss:  0.0536738532692\n",
      "Truth:  0.0754717 Pred:  0.00200423 Error:  0.0734674742617 Loss:  0.0537769450452\n",
      "Truth:  -0.1132075 Pred:  -0.00106157 Error:  -0.112145931265 Loss:  0.0540793750256\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.053810946421\n",
      "Truth:  -0.009433962 Pred:  0.00200423 Error:  -0.0114381877383 Loss:  0.0535936502226\n",
      "Truth:  -0.0754717 Pred:  0.00200423 Error:  -0.0774759257383 Loss:  0.0537154985671\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0535966692126\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0533361013163\n",
      "Truth:  -0.1226415 Pred:  0.00200423 Error:  -0.124645725738 Loss:  0.0536944411375\n",
      "Truth:  0.05660377 Pred:  0.00200423 Error:  0.0545995442617 Loss:  0.0536989666532\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0535356491362\n",
      "Truth:  -0.1132075 Pred:  -0.0367718 Error:  -0.0764356847298 Loss:  0.053649015649\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0533946078169\n",
      "Truth:  -0.0754717 Pred:  0.00200423 Error:  -0.0774759257383 Loss:  0.053512653493\n",
      "Truth:  -0.05660377 Pred:  0.00200423 Error:  -0.0586079957383 Loss:  0.053537508821\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0533789390973\n",
      "Truth:  -0.03773585 Pred:  0.00200423 Error:  -0.0397400757383 Loss:  0.0533130508686\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0530663738247\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0529574730683\n",
      "Truth:  -0.009433962 Pred:  0.00200423 Error:  -0.0114381877383 Loss:  0.0527597621857\n",
      "Truth:  -0.03773585 Pred:  0.00200423 Error:  -0.0397400757383 Loss:  0.0526980575106\n",
      "Truth:  -0.06603774 Pred:  0.00200423 Error:  -0.0680419657383 Loss:  0.0527704344362\n",
      "Truth:  -0.1320755 Pred:  -0.0090352 Error:  -0.123040295463 Loss:  0.0531003398871\n",
      "Truth:  -0.08490566 Pred:  0.000792219 Error:  -0.08569787942 Loss:  0.0532526648382\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0530143000051\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0529091695224\n",
      "Truth:  -0.03773585 Pred:  0.00200423 Error:  -0.0397400757383 Loss:  0.0528484824543\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0527450771024\n",
      "Truth:  -0.06603774 Pred:  -0.00712373 Error:  -0.0589140145112 Loss:  0.0527732457663\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0525424774935\n",
      "Truth:  -0.3301887 Pred:  -0.0456374 Error:  -0.28455134202 Loss:  0.0535922913601\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0533599126862\n",
      "Truth:  -0.05660377 Pred:  0.00200423 Error:  -0.0586079957383 Loss:  0.0533834466909\n",
      "Truth:  -0.0754717 Pred:  0.00200423 Error:  -0.0774759257383 Loss:  0.0534910024009\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0532621722824\n",
      "Truth:  0.254717 Pred:  0.00200423 Error:  0.252712774262 Loss:  0.0541446970699\n",
      "Truth:  0.1981132 Pred:  0.00200423 Error:  0.196108974262 Loss:  0.0547700903615\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0545386611307\n",
      "Truth:  -0.06603774 Pred:  0.00200423 Error:  -0.0680419657383 Loss:  0.0545976275264\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0544509949969\n",
      "Truth:  -0.06603774 Pred:  0.000647282 Error:  -0.0666850221115 Loss:  0.0545039561532\n",
      "Truth:  -0.0754717 Pred:  0.000407298 Error:  -0.0758789982119 Loss:  0.0545960899552\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0543703737997\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.054227218979\n",
      "Truth:  -0.01886792 Pred:  0.00196567 Error:  -0.0208335924762 Loss:  0.0540851184407\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0539843599546\n",
      "Truth:  -0.06603774 Pred:  0.00200423 Error:  -0.0680419657383 Loss:  0.054043674747\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0538250215999\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0536871434582\n",
      "Truth:  -0.009433962 Pred:  0.00200423 Error:  -0.0114381877383 Loss:  0.0535111061427\n",
      "Truth:  -0.06603774 Pred:  0.00200423 Error:  -0.0680419657383 Loss:  0.0535714001659\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0533583126683\n",
      "Truth:  -0.2735849 Pred:  -0.0337096 Error:  -0.23987533296 Loss:  0.0541258724226\n",
      "Truth:  -0.03773585 Pred:  0.00200423 Error:  -0.0397400757383 Loss:  0.0540669142394\n",
      "Truth:  0.1886793 Pred:  0.00200423 Error:  0.186675074262 Loss:  0.0546081720354\n",
      "Truth:  -0.06603774 Pred:  0.00200423 Error:  -0.0680419657383 Loss:  0.0546627809529\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0545641709723\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0544283160318\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0542177775166\n",
      "Truth:  -0.04716982 Pred:  -0.000988476 Error:  -0.046181343556 Loss:  0.0541856317808\n",
      "Truth:  -0.1226415 Pred:  -0.00744572 Error:  -0.115195780907 Loss:  0.054428700104\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0543329755629\n",
      "Truth:  0.1415094 Pred:  0.00200423 Error:  0.139505174262 Loss:  0.0546696245696\n",
      "Truth:  -0.1698113 Pred:  -0.000789867 Error:  -0.169021432984 Loss:  0.0551198285397\n",
      "Truth:  -0.1698113 Pred:  -0.0431674 Error:  -0.126643850466 Loss:  0.0554003149002\n",
      "Truth:  -0.03773585 Pred:  0.00200423 Error:  -0.0397400757383 Loss:  0.055339142091\n",
      "Truth:  0.08490566 Pred:  0.00200423 Error:  0.0829014342617 Loss:  0.055446388364\n",
      "Truth:  -0.05660377 Pred:  -0.00231868 Error:  -0.054285086454 Loss:  0.0554418871938\n",
      "Truth:  -0.0754717 Pred:  0.00200423 Error:  -0.0774759257383 Loss:  0.0555269607017\n",
      "Truth:  -0.1226415 Pred:  -0.00805594 Error:  -0.114585560661 Loss:  0.0557541091631\n",
      "Truth:  0.1509434 Pred:  0.00200423 Error:  0.148939174262 Loss:  0.0561111400639\n",
      "Truth:  -0.04716982 Pred:  0.00200423 Error:  -0.0491740457383 Loss:  0.0560846626046\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0559866453161\n",
      "Truth:  -0.2735849 Pred:  -0.0171714 Error:  -0.256413476118 Loss:  0.056745837857\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0566460653207\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0565115769013\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0564134291066\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0562104096911\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0560088997136\n",
      "Truth:  -0.04716982 Pred:  0.00200423 Error:  -0.0491740457383 Loss:  0.0559835854396\n",
      "Truth:  -0.05660377 Pred:  0.00200423 Error:  -0.0586079957383 Loss:  0.0559932696104\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0557947804786\n",
      "Truth:  -0.2075472 Pred:  -0.0209272 Error:  -0.186619998043 Loss:  0.0562739937297\n",
      "Truth:  -0.06603774 Pred:  0.000409335 Error:  -0.0664470752472 Loss:  0.0563111217644\n",
      "Truth:  -0.009433962 Pred:  0.00200423 Error:  -0.0114381877383 Loss:  0.0561479474589\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0560543176338\n",
      "Truth:  -0.05660377 Pred:  0.00200423 Error:  -0.0586079957383 Loss:  0.0560635366883\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0559369489511\n",
      "Truth:  -0.01886792 Pred:  0.00194365 Error:  -0.0208115701898 Loss:  0.0558110515362\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0556862697298\n",
      "Truth:  -0.06603774 Pred:  -0.00209098 Error:  -0.0639467624952 Loss:  0.0557156665012\n",
      "Truth:  -0.08490566 Pred:  -0.0033664 Error:  -0.0815392618406 Loss:  0.0558072395343\n",
      "Truth:  -0.03773585 Pred:  0.00200423 Error:  -0.0397400757383 Loss:  0.0557504651039\n",
      "Truth:  0.0 Pred:  -0.00448092 Error:  0.00448092492297 Loss:  0.055569938554\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0553819886845\n",
      "Truth:  -0.01886792 Pred:  -0.00204646 Error:  -0.0168214597142 Loss:  0.0552471616601\n",
      "Truth:  -0.1509434 Pred:  0.00154411 Error:  -0.152487512572 Loss:  0.0555859782138\n",
      "Truth:  -0.1226415 Pred:  -0.00687033 Error:  -0.115771172483 Loss:  0.0557949545828\n",
      "Truth:  -0.03773585 Pred:  0.00200423 Error:  -0.0397400757383 Loss:  0.0557394013688\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0556191694528\n",
      "Truth:  0.0 Pred:  0.000289579 Error:  -0.000289579271339 Loss:  0.0554290334041\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.055246071734\n",
      "Truth:  -0.1509434 Pred:  -0.0103304 Error:  -0.140613024716 Loss:  0.0555374265223\n",
      "Truth:  -0.009433962 Pred:  0.00200423 Error:  -0.0114381877383 Loss:  0.0553874291115\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0552064691001\n",
      "Truth:  0.09433963 Pred:  0.00200423 Error:  0.0923354042617 Loss:  0.0553319046918\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0552476427762\n",
      "Truth:  -0.1603774 Pred:  -0.00362395 Error:  -0.156753447842 Loss:  0.0555882662831\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0554090554452\n",
      "Truth:  -0.1132075 Pred:  -0.0407983 Error:  -0.0724091749084 Loss:  0.05546572251\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.055288109564\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0551741494188\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0549986711229\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0548243472894\n",
      "Truth:  -0.4811321 Pred:  0.00200423 Error:  -0.483136325738 Loss:  0.056228648858\n",
      "Truth:  -0.09433963 Pred:  0.000183609 Error:  -0.0945232388486 Loss:  0.056353794577\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0561767601508\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0560621347793\n",
      "Truth:  -0.1509434 Pred:  0.00200423 Error:  -0.152947625738 Loss:  0.0563756800575\n",
      "Truth:  0.1037736 Pred:  0.00200423 Error:  0.101769374262 Loss:  0.0565221113291\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0564074812147\n",
      "Truth:  -0.06603774 Pred:  0.00161967 Error:  -0.067657405185 Loss:  0.0564435386633\n",
      "Truth:  0.01886792 Pred:  0.00200423 Error:  0.0168636942617 Loss:  0.0563170854863\n",
      "Truth:  -0.05660377 Pred:  0.00159146 Error:  -0.0581952310121 Loss:  0.0563230668415\n",
      "Truth:  -0.1132075 Pred:  0.00200423 Error:  -0.115211725738 Loss:  0.056510014965\n",
      "Truth:  -0.1509434 Pred:  0.00200423 Error:  -0.152947625738 Loss:  0.0568151972775\n",
      "Truth:  -0.03773585 Pred:  0.00144667 Error:  -0.0391825159632 Loss:  0.0567595736772\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0566467201302\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0564754270443\n",
      "Truth:  -0.009433962 Pred:  0.00200423 Error:  -0.0114381877383 Loss:  0.0563346856715\n",
      "Truth:  -0.05660377 Pred:  0.00200423 Error:  -0.0586079957383 Loss:  0.0563417676343\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0561730175042\n",
      "Truth:  -0.06603774 Pred:  -0.00503677 Error:  -0.0610009668399 Loss:  0.0561879647158\n",
      "Truth:  -0.2735849 Pred:  -0.0178635 Error:  -0.255721447565 Loss:  0.0568038087987\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0566351946969\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0564676150375\n",
      "Truth:  -0.1698113 Pred:  0.00200423 Error:  -0.171815525738 Loss:  0.0568203609418\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0567395248284\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0565731561382\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.056407795743\n",
      "Truth:  -0.0754717 Pred:  0.00130392 Error:  -0.0767756173791 Loss:  0.0564693299473\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0563052784286\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0561422122044\n",
      "Truth:  0.1132075 Pred:  0.00200423 Error:  0.111203274262 Loss:  0.0563070656836\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0562294509076\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0561242208327\n",
      "Truth:  0.009433962 Pred:  -0.00695223 Error:  0.0163861890933 Loss:  0.0560063038246\n",
      "Truth:  0.03773585 Pred:  -0.0233093 Error:  0.0610451888379 Loss:  0.0560212117684\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0558618696267\n",
      "Truth:  0.3113208 Pred:  0.00200423 Error:  0.309316574262 Loss:  0.0566073246403\n",
      "Truth:  -0.1226415 Pred:  -0.0209176 Error:  -0.101723868314 Loss:  0.05673963122\n",
      "Truth:  -0.04716982 Pred:  0.00200423 Error:  -0.0491740457383 Loss:  0.0567175096251\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0565579956779\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0563994091373\n",
      "Truth:  0.09433963 Pred:  0.00200423 Error:  0.0923354042617 Loss:  0.0565035714421\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0563460588822\n",
      "Truth:  -0.01886792 Pred:  0.00053693 Error:  -0.019404849816 Loss:  0.0562396000665\n",
      "Truth:  0.08490566 Pred:  0.00205071 Error:  0.0828549540474 Loss:  0.0563160809687\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.056160459607\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0560865900531\n",
      "Truth:  0.1603774 Pred:  0.00200423 Error:  0.158373174262 Loss:  0.0563780048229\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0563039369562\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0561501134117\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0560504581358\n",
      "Truth:  -0.1509434 Pred:  -0.0176692 Error:  -0.1332741693 Loss:  0.0562679897165\n",
      "Truth:  -0.1226415 Pred:  0.00200423 Error:  -0.124645725738 Loss:  0.0564600620087\n",
      "Truth:  -0.1320755 Pred:  -0.0011629 Error:  -0.130912599643 Loss:  0.0566686125343\n",
      "Truth:  -0.1226415 Pred:  -0.0192998 Error:  -0.103341687385 Loss:  0.0567989842518\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0566463526125\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0564945689267\n",
      "Truth:  0.03773585 Pred:  0.00200423 Error:  0.0357316242617 Loss:  0.0564370538445\n",
      "Truth:  -0.2735849 Pred:  -0.0214497 Error:  -0.252135237255 Loss:  0.0569776565611\n",
      "Truth:  0.0 Pred:  0.00189235 Error:  -0.00189235445578 Loss:  0.0568259064176\n",
      "Truth:  0.0 Pred:  -0.026821 Error:  0.0268210470676 Loss:  0.0567434754853\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.056593504938\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0565216814703\n",
      "Truth:  -0.3207547 Pred:  -0.0215882 Error:  -0.299166501159 Loss:  0.0571828390171\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.057084168655\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.056934900517\n",
      "Truth:  0.02830189 Pred:  0.00200423 Error:  0.0262976642617 Loss:  0.0568520971757\n",
      "Truth:  -0.05660377 Pred:  0.00200423 Error:  -0.0586079957383 Loss:  0.056856830056\n",
      "Truth:  -0.0754717 Pred:  0.000456267 Error:  -0.0759279670365 Loss:  0.0569080965532\n",
      "Truth:  -0.1226415 Pred:  -0.00706288 Error:  -0.115578622472 Loss:  0.0570653901884\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0569181678235\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0567717306447\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0566260723869\n",
      "Truth:  -0.08490566 Pred:  0.00200423 Error:  -0.0869098857383 Loss:  0.0567064008043\n",
      "Truth:  -0.1226415 Pred:  -0.0344453 Error:  -0.0881962328293 Loss:  0.0567897072382\n",
      "Truth:  0.3207547 Pred:  0.00383411 Error:  0.316920590945 Loss:  0.057476068409\n",
      "Truth:  0.01886792 Pred:  0.00200423 Error:  0.0168636942617 Loss:  0.0573691937402\n",
      "Truth:  -0.254717 Pred:  -0.0168105 Error:  -0.237906500868 Loss:  0.05784304494\n",
      "Truth:  -0.03773585 Pred:  0.00200423 Error:  -0.0397400757383 Loss:  0.0577956549682\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0577238807144\n",
      "Truth:  -0.01886792 Pred:  -0.000187243 Error:  -0.0186806766651 Loss:  0.0576222057038\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0575512548208\n",
      "Truth:  -0.04716982 Pred:  0.00200423 Error:  -0.0491740457383 Loss:  0.0575295522066\n",
      "Truth:  -0.08490566 Pred:  0.00200423 Error:  -0.0869098857383 Loss:  0.0576054703811\n",
      "Truth:  -0.05660377 Pred:  -0.000152996 Error:  -0.0564507739573 Loss:  0.0576024943594\n",
      "Truth:  -0.04716982 Pred:  -0.00117762 Error:  -0.0459922049796 Loss:  0.0575726478571\n",
      "Truth:  -0.1509434 Pred:  -0.00714917 Error:  -0.14379423171 Loss:  0.0577937288414\n",
      "Truth:  -0.009433962 Pred:  -0.00148749 Error:  -0.007946467088 Loss:  0.0576662422384\n",
      "Truth:  0.02830189 Pred:  0.00200423 Error:  0.0262976642617 Loss:  0.0575862203558\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0575168053313\n",
      "Truth:  0.0 Pred:  -0.0369048 Error:  0.0369047634304 Loss:  0.0574644905042\n",
      "Truth:  -0.1981132 Pred:  -0.00992523 Error:  -0.188187967731 Loss:  0.0577954360162\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0576545491215\n",
      "Truth:  -0.05660377 Pred:  0.00200423 Error:  -0.0586079957383 Loss:  0.0576569507503\n",
      "Truth:  0.009433962 Pred:  0.00200423 Error:  0.00742973626165 Loss:  0.0575307517189\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0574388755134\n",
      "Truth:  -0.01886792 Pred:  0.00186038 Error:  -0.0207282950505 Loss:  0.0573470990623\n",
      "Truth:  -0.1037736 Pred:  0.00200423 Error:  -0.105777825738 Loss:  0.0574678739418\n",
      "Truth:  0.0 Pred:  -0.000164343 Error:  0.000164342578501 Loss:  0.0573253278438\n",
      "Truth:  -0.05660377 Pred:  0.0019763 Error:  -0.0585800667713 Loss:  0.0573284413399\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0571915002122\n",
      "Truth:  -0.04716982 Pred:  0.00200423 Error:  -0.0491740457383 Loss:  0.0571717040283\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0570358235399\n",
      "Truth:  -0.0754717 Pred:  0.00169404 Error:  -0.0771657366477 Loss:  0.0570852827859\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0569502801951\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0568159377637\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0566822506612\n",
      "Truth:  -0.06603774 Pred:  0.00200423 Error:  -0.0680419657383 Loss:  0.0567098898706\n",
      "Truth:  -0.02830189 Pred:  -0.0121799 Error:  -0.0161220244982 Loss:  0.0566113756342\n",
      "Truth:  0.03773585 Pred:  0.00200423 Error:  0.0357316242617 Loss:  0.0565608193355\n",
      "Truth:  0.254717 Pred:  0.00200423 Error:  0.252712774262 Loss:  0.0570346163281\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0569020129773\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0567700471426\n",
      "Truth:  -0.05660377 Pred:  0.00200423 Error:  -0.0586079957383 Loss:  0.0567744546932\n",
      "Truth:  -0.05660377 Pred:  0.00200423 Error:  -0.0586079957383 Loss:  0.056778841155\n",
      "Truth:  -0.1320755 Pred:  -0.00268821 Error:  -0.129387286748 Loss:  0.0569521310013\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0568213026554\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0567583212138\n",
      "Truth:  -0.1226415 Pred:  -0.0336306 Error:  -0.0890109240153 Loss:  0.0568347491825\n",
      "Truth:  -0.04716982 Pred:  0.00200423 Error:  -0.0491740457383 Loss:  0.0568166387725\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.056754113954\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0566252906876\n",
      "Truth:  -0.04716982 Pred:  0.00200423 Error:  -0.0491740457383 Loss:  0.0566077995023\n",
      "Truth:  -0.2641509 Pred:  0.00200423 Error:  -0.266155125738 Loss:  0.0570985426551\n",
      "Truth:  -0.1226415 Pred:  -0.00648046 Error:  -0.116161039014 Loss:  0.0572365391419\n",
      "Truth:  -0.1226415 Pred:  -0.0272937 Error:  -0.0953478067257 Loss:  0.0573253765955\n",
      "Truth:  0.01886792 Pred:  0.00200423 Error:  0.0168636942617 Loss:  0.0572312796598\n",
      "Truth:  -0.1981132 Pred:  -0.00620463 Error:  -0.191908566026 Loss:  0.0575437559623\n",
      "Truth:  -0.02830189 Pred:  0.0016385 Error:  -0.0299403861782 Loss:  0.057479859273\n",
      "Truth:  -0.09433963 Pred:  0.000530344 Error:  -0.0948699743177 Loss:  0.0575662105779\n",
      "Truth:  -0.1226415 Pred:  0.00200423 Error:  -0.124645725738 Loss:  0.0577207716728\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0576360621879\n",
      "Truth:  -0.1132075 Pred:  0.00200423 Error:  -0.115211725738 Loss:  0.0577681164621\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0576836863231\n",
      "Truth:  -0.1415094 Pred:  0.00200423 Error:  -0.143513625738 Loss:  0.0578796450889\n",
      "Truth:  -0.1509434 Pred:  -0.0226486 Error:  -0.128294815902 Loss:  0.0580400441112\n",
      "Truth:  0.02830189 Pred:  0.00200423 Error:  0.0262976642617 Loss:  0.0579679023389\n",
      "Truth:  -0.1792453 Pred:  -0.0130691 Error:  -0.166176151825 Loss:  0.0582132725191\n",
      "Truth:  -0.2075472 Pred:  -0.0233704 Error:  -0.184176781302 Loss:  0.0584982578331\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0584133230428\n",
      "Truth:  0.0 Pred:  -0.0303136 Error:  0.0303135886788 Loss:  0.0583500353528\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0582234155559\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0580973635608\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0579718755567\n",
      "Truth:  -0.02830189 Pred:  0.00154038 Error:  -0.0298422670376 Loss:  0.057909086252\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0577845765404\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0576606202053\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0575790493084\n",
      "Truth:  -0.0754717 Pred:  0.00200423 Error:  -0.0774759257383 Loss:  0.0576230689466\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0575002900432\n",
      "Truth:  -0.05660377 Pred:  -0.000428484 Error:  -0.0561752857129 Loss:  0.0574973715315\n",
      "Truth:  -0.08490566 Pred:  0.00200423 Error:  -0.0869098857383 Loss:  0.0575620144199\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.057440177164\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0573601595897\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0572804914372\n",
      "Truth:  -0.04716982 Pred:  0.00200423 Error:  -0.0491740457383 Loss:  0.0572628303355\n",
      "Truth:  -0.03773585 Pred:  0.00200423 Error:  -0.0397400757383 Loss:  0.0572247373907\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0571663455867\n",
      "Truth:  -0.06603774 Pred:  0.000955089 Error:  -0.0669928286462 Loss:  0.0571876150305\n",
      "Truth:  0.0 Pred:  -0.00759102 Error:  0.00759101752192 Loss:  0.0570804949495\n",
      "Truth:  -0.05660377 Pred:  0.00200423 Error:  -0.0586079957383 Loss:  0.0570837869771\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0569653363078\n",
      "Truth:  -0.245283 Pred:  -0.0423625 Error:  -0.20292046649 Loss:  0.0572785447416\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0572207879343\n",
      "Truth:  -0.06603774 Pred:  0.00200423 Error:  -0.0680419657383 Loss:  0.0572439101091\n",
      "Truth:  -0.2075472 Pred:  -0.0190781 Error:  -0.188469079411 Loss:  0.0575237079115\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0574055813537\n",
      "Truth:  -0.0754717 Pred:  0.00200423 Error:  -0.0774759257383 Loss:  0.0574481935498\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0573307275163\n",
      "Truth:  0.2641509 Pred:  0.00200423 Error:  0.262146674262 Loss:  0.0577637422029\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0576461060922\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0575289652915\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0574717744311\n",
      "Truth:  -0.1886793 Pred:  -0.00587435 Error:  -0.182804949333 Loss:  0.0577345274183\n",
      "Truth:  -0.02830189 Pred:  -0.00819787 Error:  -0.0201040171005 Loss:  0.0576558025013\n",
      "Truth:  -0.02830189 Pred:  -0.00148599 Error:  -0.0268158987083 Loss:  0.0575914185685\n",
      "Truth:  -0.08490566 Pred:  0.00200423 Error:  -0.0869098857383 Loss:  0.0576524987085\n",
      "Truth:  -0.04716982 Pred:  0.00200423 Error:  -0.0491740457383 Loss:  0.0576348719871\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0575194557086\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0574045173443\n",
      "Truth:  -0.2735849 Pred:  -0.016394 Error:  -0.257190865972 Loss:  0.0578172990563\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0577411234824\n",
      "Truth:  -0.09433963 Pred:  -0.00112889 Error:  -0.0932107422423 Loss:  0.0578141062371\n",
      "Truth:  -0.09433963 Pred:  0.00200423 Error:  -0.0963438557383 Loss:  0.0578932227658\n",
      "Truth:  -0.04716982 Pred:  0.00200423 Error:  -0.0491740457383 Loss:  0.0578753555998\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0577610997105\n",
      "Truth:  -0.1886793 Pred:  0.00200423 Error:  -0.190683525738 Loss:  0.0580323699677\n",
      "Truth:  -0.05660377 Pred:  0.00170294 Error:  -0.0583067142824 Loss:  0.0580329287138\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0579190492362\n",
      "Truth:  0.1981132 Pred:  0.00200423 Error:  0.196108974262 Loss:  0.0581993533437\n",
      "Truth:  -0.1981132 Pred:  -0.0246364 Error:  -0.173476782373 Loss:  0.0584327084632\n",
      "Truth:  -0.1132075 Pred:  0.000696416 Error:  -0.113903916013 Loss:  0.0585447715088\n",
      "Truth:  -0.03773585 Pred:  0.00200423 Error:  -0.0397400757383 Loss:  0.0585068588157\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0584501168779\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0583746591045\n",
      "Truth:  -0.03773585 Pred:  -0.0267628 Error:  -0.0109730701979 Loss:  0.0582796659404\n",
      "Truth:  -0.05660377 Pred:  0.00200423 Error:  -0.0586079957383 Loss:  0.0582803226\n",
      "Truth:  -0.04716982 Pred:  0.00200423 Error:  -0.0491740457383 Loss:  0.0582621463987\n",
      "Truth:  -0.05660377 Pred:  -0.00012698 Error:  -0.0564767897549 Loss:  0.0582585899113\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0581467522092\n",
      "Truth:  -0.05660377 Pred:  -0.00108958 Error:  -0.0555141940667 Loss:  0.0581415288796\n",
      "Truth:  -0.1415094 Pred:  0.00200423 Error:  -0.143513625738 Loss:  0.0583105825367\n",
      "Truth:  0.0 Pred:  -0.000139228 Error:  0.000139228068292 Loss:  0.0581956193856\n",
      "Truth:  -0.1698113 Pred:  0.00200423 Error:  -0.171815525738 Loss:  0.058419721765\n",
      "Truth:  -0.1037736 Pred:  0.00200423 Error:  -0.105777825738 Loss:  0.0585129463791\n",
      "Truth:  -0.1132075 Pred:  0.00200423 Error:  -0.115211725738 Loss:  0.0586243388729\n",
      "Truth:  -0.03773585 Pred:  0.00200423 Error:  -0.0397400757383 Loss:  0.058587310906\n",
      "Truth:  -0.09433963 Pred:  -0.0060104 Error:  -0.0883292298687 Loss:  0.0586455142699\n",
      "Truth:  -0.1132075 Pred:  -0.0310064 Error:  -0.082201063205 Loss:  0.0586915212014\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0585810196508\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0585260101101\n",
      "Truth:  0.0 Pred:  -0.00514382 Error:  0.00514382123947 Loss:  0.0584223553745\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.058367866538\n",
      "Truth:  -0.03773585 Pred:  0.00200423 Error:  -0.0397400757383 Loss:  0.0583318359948\n",
      "Truth:  -0.04716982 Pred:  0.00200423 Error:  -0.0491740457383 Loss:  0.0583141568631\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0582056598859\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0580975802049\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.057989915417\n",
      "Truth:  -0.2830189 Pred:  -0.0255626 Error:  -0.25745625227 Loss:  0.0583720348363\n",
      "Truth:  0.3773585 Pred:  0.00200423 Error:  0.375354274262 Loss:  0.0589781194241\n",
      "Truth:  -0.3396226 Pred:  -0.0156897 Error:  -0.323932873081 Loss:  0.059483758267\n",
      "Truth:  0.0 Pred:  -3.40799e-05 Error:  3.40798869729e-05 Loss:  0.0593705207843\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0592614593869\n",
      "Truth:  -0.03773585 Pred:  -0.000130723 Error:  -0.037605127468 Loss:  0.059220365778\n",
      "Truth:  -0.1226415 Pred:  -0.0520092 Error:  -0.0706322751083 Loss:  0.0592419792426\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0591337793305\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0590793875124\n",
      "Truth:  -0.04716982 Pred:  -0.000522292 Error:  -0.0466475280137 Loss:  0.0590559753476\n",
      "Truth:  -0.05660377 Pred:  0.00200423 Error:  -0.0586079957383 Loss:  0.0590551332807\n",
      "Truth:  -0.1226415 Pred:  -0.0104231 Error:  -0.112218381751 Loss:  0.0591548767112\n",
      "Truth:  -0.05660377 Pred:  0.00200423 Error:  -0.0586079957383 Loss:  0.0591538525895\n",
      "Truth:  -0.0754717 Pred:  0.00200423 Error:  -0.0774759257383 Loss:  0.0591880994552\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.059116614467\n",
      "Truth:  -0.1698113 Pred:  -0.0332415 Error:  -0.136569785883 Loss:  0.0592608475608\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0591894930965\n",
      "Truth:  -0.1037736 Pred:  -0.00771893 Error:  -0.0960546709104 Loss:  0.0592578886026\n",
      "Truth:  -0.1509434 Pred:  0.00200423 Error:  -0.152947625738 Loss:  0.0594313881159\n",
      "Truth:  0.09433963 Pred:  0.00200423 Error:  0.0923354042617 Loss:  0.0594922088481\n",
      "Truth:  -0.09433963 Pred:  -0.000749064 Error:  -0.0935905658563 Loss:  0.0595551209459\n",
      "Truth:  -0.1509434 Pred:  -0.00102055 Error:  -0.149922849971 Loss:  0.0597215440196\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0596501296845\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0595789774204\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0594735291573\n",
      "Truth:  -0.01886792 Pred:  0.00143864 Error:  -0.0203065619207 Loss:  0.0594019259265\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0592971855977\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0591928268365\n",
      "Truth:  -0.03773585 Pred:  0.00200423 Error:  -0.0397400757383 Loss:  0.0591574581982\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0590879748725\n",
      "Truth:  -0.06603774 Pred:  -0.00679361 Error:  -0.0592441329737 Loss:  0.0590882577676\n",
      "Truth:  -0.1886793 Pred:  -0.0171822 Error:  -0.171497069051 Loss:  0.0592915286741\n",
      "Truth:  0.1886793 Pred:  0.00200423 Error:  0.186675074262 Loss:  0.0595214628719\n",
      "Truth:  -0.03773585 Pred:  0.00200423 Error:  -0.0397400757383 Loss:  0.0594858207329\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0593824365692\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.059330234916\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0592275001326\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0591757624145\n",
      "Truth:  -0.1792453 Pred:  -0.0155845 Error:  -0.163660809738 Loss:  0.0593623428562\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0593105492249\n",
      "Truth:  -0.2075472 Pred:  -0.0491159 Error:  -0.158431292599 Loss:  0.0594869206544\n",
      "Truth:  -0.254717 Pred:  -0.0178407 Error:  -0.236876268111 Loss:  0.0598019994243\n",
      "Truth:  -0.1132075 Pred:  0.00200423 Error:  -0.115211725738 Loss:  0.0599002436199\n",
      "Truth:  -0.1132075 Pred:  -0.0109502 Error:  -0.102257287633 Loss:  0.0599752118394\n",
      "Truth:  -0.06603774 Pred:  -0.000748386 Error:  -0.0652893543247 Loss:  0.0599846007837\n",
      "Truth:  -0.1509434 Pred:  -0.0343722 Error:  -0.116571237926 Loss:  0.0600844008492\n",
      "Truth:  0.0 Pred:  -0.00898946 Error:  0.0089894561097 Loss:  0.0599944449606\n",
      "Truth:  0.009433962 Pred:  0.000441754 Error:  0.00899220834526 Loss:  0.0599048100984\n",
      "Truth:  -0.1320755 Pred:  -0.00276962 Error:  -0.129305883102 Loss:  0.0600265663668\n",
      "Truth:  -0.05660377 Pred:  -0.000862816 Error:  -0.0557409538842 Loss:  0.0600190609159\n",
      "Truth:  -0.08490566 Pred:  0.00200423 Error:  -0.0869098857383 Loss:  0.0600660728474\n",
      "Truth:  0.3113208 Pred:  0.00200423 Error:  0.309316574262 Loss:  0.0605010649964\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0603991541267\n",
      "Truth:  0.1509434 Pred:  0.00200423 Error:  0.148939174262 Loss:  0.0605531367704\n",
      "Truth:  -0.08490566 Pred:  0.000145879 Error:  -0.085051538992 Loss:  0.0605956687187\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0604941237568\n",
      "Truth:  -0.1037736 Pred:  0.00200423 Error:  -0.105777825738 Loss:  0.060572469262\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0605039022093\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0604518370602\n",
      "Truth:  -0.06603774 Pred:  0.00200423 Error:  -0.0680419657383 Loss:  0.060464900965\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0603644530694\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0602643497635\n",
      "Truth:  -0.1792453 Pred:  -0.0156808 Error:  -0.163564502602 Loss:  0.0604412335869\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0603897205649\n",
      "Truth:  -0.08490566 Pred:  0.000114173 Error:  -0.0850198330463 Loss:  0.0604317514736\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0603322156546\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0602330183928\n",
      "Truth:  -0.01886792 Pred:  -0.000328935 Error:  -0.0185389850157 Loss:  0.0601622305603\n",
      "Truth:  -0.1509434 Pred:  0.00200423 Error:  -0.152947625738 Loss:  0.0603194939419\n",
      "Truth:  0.04716982 Pred:  0.00200423 Error:  0.0451655942617 Loss:  0.0602938528257\n",
      "Truth:  -0.04716982 Pred:  -0.0004969 Error:  -0.0466729198252 Loss:  0.060270844493\n",
      "Truth:  -0.1981132 Pred:  -0.0113901 Error:  -0.186723064376 Loss:  0.0604840860105\n",
      "Truth:  0.1886793 Pred:  0.00200423 Error:  0.186675074262 Loss:  0.0606965287516\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0606454524272\n",
      "Truth:  0.0 Pred:  -0.0381539 Error:  0.0381538644433 Loss:  0.0606077148635\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0605095515651\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0604432682778\n",
      "Truth:  0.0 Pred:  -0.026883 Error:  0.0268829651177 Loss:  0.0603872410605\n",
      "Truth:  0.1509434 Pred:  0.00200423 Error:  0.148939174262 Loss:  0.0605348276158\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0604688331368\n",
      "Truth:  0.2641509 Pred:  0.00200423 Error:  0.262146674262 Loss:  0.060803846162\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0607532694946\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0606560028659\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0606058377632\n",
      "Truth:  -0.2830189 Pred:  -0.0473831 Error:  -0.235635781579 Loss:  0.0608946660533\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.060797647206\n",
      "Truth:  -0.2075472 Pred:  -0.0402902 Error:  -0.167256974703 Loss:  0.0609727447841\n",
      "Truth:  -0.04716982 Pred:  0.00200423 Error:  -0.0491740457383 Loss:  0.0609533708941\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0608567329512\n",
      "Truth:  0.08490566 Pred:  0.00200423 Error:  0.0829014342617 Loss:  0.0608928126587\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0607965894775\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0607006802381\n",
      "Truth:  -0.04716982 Pred:  0.00200423 Error:  -0.0491740457383 Loss:  0.0606819072177\n",
      "Truth:  -0.1320755 Pred:  0.000581782 Error:  -0.132657281613 Loss:  0.0607989403468\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0607034943816\n",
      "Truth:  -0.1320755 Pred:  -0.0139748 Error:  -0.11810072444 Loss:  0.0607965206864\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.060701387523\n",
      "Truth:  -0.009433962 Pred:  0.00200423 Error:  -0.0114381877383 Loss:  0.060621802386\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0605576900366\n",
      "Truth:  -0.05660377 Pred:  0.00200423 Error:  -0.0586079957383 Loss:  0.0605545504323\n",
      "Truth:  -0.05660377 Pred:  0.00200423 Error:  -0.0586079957383 Loss:  0.0605514209231\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0604574446869\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0603637696565\n",
      "Truth:  0.01886792 Pred:  0.00200423 Error:  0.0168636942617 Loss:  0.0602941695359\n",
      "Truth:  -0.04716982 Pred:  0.00172582 Error:  -0.0488956393111 Loss:  0.0602759610211\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0602131160207\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0601204267049\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0600280321088\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0599359308289\n",
      "Truth:  -0.1226415 Pred:  -0.0361209 Error:  -0.0865206121542 Loss:  0.0599780618611\n",
      "Truth:  -0.2735849 Pred:  0.00200423 Error:  -0.275589125738 Loss:  0.0603192186077\n",
      "Truth:  -0.0754717 Pred:  0.00200423 Error:  -0.0774759257383 Loss:  0.0603463224104\n",
      "Truth:  -0.1320755 Pred:  -0.0147592 Error:  -0.117316334765 Loss:  0.0604361804741\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0603441616477\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0602524321888\n",
      "Truth:  -0.009433962 Pred:  0.00200423 Error:  -0.0114381877383 Loss:  0.0601758007218\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0600846227046\n",
      "Truth:  -0.1037736 Pred:  0.00200423 Error:  -0.105777825738 Loss:  0.0601561300646\n",
      "Truth:  -0.06603774 Pred:  -0.000916545 Error:  -0.0651211951863 Loss:  0.0601638879788\n",
      "Truth:  -0.04716982 Pred:  0.00200423 Error:  -0.0491740457383 Loss:  0.0601467431391\n",
      "Truth:  -0.0754717 Pred:  0.00200423 Error:  -0.0774759257383 Loss:  0.0601737356354\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0600832698346\n",
      "Truth:  -0.06603774 Pred:  0.000565095 Error:  -0.0666028348733 Loss:  0.0600933933828\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0600033326578\n",
      "Truth:  -0.08490566 Pred:  -0.000566831 Error:  -0.0843388286758 Loss:  0.0600410037043\n",
      "Truth:  0.245283 Pred:  0.00200423 Error:  0.243278774262 Loss:  0.0603242150962\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0602342151126\n",
      "Truth:  -0.245283 Pred:  -0.0150383 Error:  -0.230244685725 Loss:  0.0604961726944\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0604061850837\n",
      "Truth:  -0.009433962 Pred:  0.00200423 Error:  -0.0114381877383 Loss:  0.0603309654257\n",
      "Truth:  -0.06603774 Pred:  0.00175596 Error:  -0.0677937020319 Loss:  0.0603424113408\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0602530726186\n",
      "Truth:  -0.009433962 Pred:  0.00200423 Error:  -0.0114381877383 Loss:  0.0601784321218\n",
      "Truth:  -0.06603774 Pred:  0.00200423 Error:  -0.0680419657383 Loss:  0.0601904375166\n",
      "Truth:  -0.1509434 Pred:  0.00200423 Error:  -0.152947625738 Loss:  0.0603318356694\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0602717752586\n",
      "Truth:  -0.1037736 Pred:  0.00200423 Error:  -0.105777825738 Loss:  0.06034093339\n",
      "Truth:  -0.04716982 Pred:  0.00200423 Error:  -0.0491740457383 Loss:  0.0603239881887\n",
      "Truth:  -0.04716982 Pred:  0.00200423 Error:  -0.0491740457383 Loss:  0.0603070943365\n",
      "Truth:  -0.1037736 Pred:  -0.0133958 Error:  -0.0903777596979 Loss:  0.0603525870224\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0602644475038\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0602050322674\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0601457959925\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0600583650598\n",
      "Truth:  0.1509434 Pred:  0.00200423 Error:  0.148939174262 Loss:  0.0601918197282\n",
      "Truth:  -0.1037736 Pred:  -0.013066 Error:  -0.0907075661673 Loss:  0.0602375704725\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.060178640196\n",
      "Truth:  -0.08490566 Pred:  -0.0133664 Error:  -0.0715392122383 Loss:  0.0601956216191\n",
      "Truth:  -0.04716982 Pred:  -0.000922919 Error:  -0.0462469012146 Loss:  0.0601748026334\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0600881102684\n",
      "Truth:  -0.0754717 Pred:  0.00200423 Error:  -0.0774759257383 Loss:  0.0601139849938\n",
      "Truth:  0.1603774 Pred:  0.00200423 Error:  0.158373174262 Loss:  0.0602599867609\n",
      "Truth:  -0.1037736 Pred:  -0.00661404 Error:  -0.0971595566227 Loss:  0.0603147338972\n",
      "Truth:  0.0754717 Pred:  0.00200423 Error:  0.0734674742617 Loss:  0.0603342194385\n",
      "Truth:  -0.245283 Pred:  0.00200423 Error:  -0.247287225738 Loss:  0.0606107771401\n",
      "Truth:  -0.05660377 Pred:  -0.000340946 Error:  -0.0562628235156 Loss:  0.0606043547566\n",
      "Truth:  -0.1698113 Pred:  -0.030066 Error:  -0.139745256861 Loss:  0.0607210817509\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0606346062634\n",
      "Truth:  -0.1603774 Pred:  -0.0173226 Error:  -0.143054774035 Loss:  0.0607558123925\n",
      "Truth:  -0.1226415 Pred:  -0.0175118 Error:  -0.105129718695 Loss:  0.0608209723136\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0607762291222\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0606901793368\n",
      "Truth:  -0.09433963 Pred:  0.00121301 Error:  -0.0955526383051 Loss:  0.0607411478441\n",
      "Truth:  -0.2264151 Pred:  0.00200423 Error:  -0.228419325738 Loss:  0.0609859335052\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0608999543394\n",
      "Truth:  0.08490566 Pred:  0.00200423 Error:  0.0829014342617 Loss:  0.0609319797833\n",
      "Truth:  -0.09433963 Pred:  0.00147384 Error:  -0.0958134662515 Loss:  0.0609826796182\n",
      "Truth:  -0.03773585 Pred:  0.00200423 Error:  -0.0397400757383 Loss:  0.0609518485531\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0608664172157\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.060781233147\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.06073719396\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0606932818702\n",
      "Truth:  0.0 Pred:  -0.000163222 Error:  0.000163221731782 Loss:  0.0606060627634\n",
      "Truth:  -0.03773585 Pred:  0.00200423 Error:  -0.0397400757383 Loss:  0.0605760397604\n",
      "Truth:  -0.05660377 Pred:  -0.00011334 Error:  -0.056490430371 Loss:  0.060570169632\n",
      "Truth:  -0.0754717 Pred:  0.00200423 Error:  -0.0774759257383 Loss:  0.0605944246623\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0605375159532\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0604807700731\n",
      "Truth:  0.08490566 Pred:  0.00200423 Error:  0.0829014342617 Loss:  0.0605127995934\n",
      "Truth:  0.01886792 Pred:  0.00200423 Error:  0.0168636942617 Loss:  0.0604505326813\n",
      "Truth:  -0.09433963 Pred:  0.00200423 Error:  -0.0963438557383 Loss:  0.0605016627712\n",
      "Truth:  -0.05660377 Pred:  0.000442251 Error:  -0.0570460206317 Loss:  0.0604967472062\n",
      "Truth:  -0.02830189 Pred:  -0.000752933 Error:  -0.0275489573751 Loss:  0.0604499463684\n",
      "Truth:  -0.08490566 Pred:  0.00200423 Error:  -0.0869098857383 Loss:  0.0604874781973\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0604313658284\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0603487248947\n",
      "Truth:  -0.0754717 Pred:  -0.00711681 Error:  -0.0683548867874 Loss:  0.060360033033\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.060317643869\n",
      "Truth:  -0.009433962 Pred:  0.00200423 Error:  -0.0114381877383 Loss:  0.0602487995646\n",
      "Truth:  0.04716982 Pred:  0.00200423 Error:  0.0451655942617 Loss:  0.0602275854924\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0601458111107\n",
      "Truth:  -0.1037736 Pred:  -0.000409018 Error:  -0.103364581752 Loss:  0.0602064264973\n",
      "Truth:  -0.04716982 Pred:  0.00200423 Error:  -0.0491740457383 Loss:  0.0601909749837\n",
      "Truth:  -0.0754717 Pred:  0.00200423 Error:  -0.0774759257383 Loss:  0.06021514974\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0601602014103\n",
      "Truth:  -0.04716982 Pred:  0.00200423 Error:  -0.0491740457383 Loss:  0.0601448790175\n",
      "Truth:  -0.05660377 Pred:  0.00200423 Error:  -0.0586079957383 Loss:  0.0601427385115\n",
      "Truth:  -0.1415094 Pred:  0.00200423 Error:  -0.143513625738 Loss:  0.0602586924576\n",
      "Truth:  -0.254717 Pred:  -0.0426655 Error:  -0.212051499806 Loss:  0.0604695158012\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0603884266332\n",
      "Truth:  0.3867925 Pred:  0.00200423 Error:  0.384788274262 Loss:  0.0608377339014\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0607563597546\n",
      "Truth:  -0.09433963 Pred:  -7.30308e-05 Error:  -0.0942665991762 Loss:  0.0608026446157\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0607605804379\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0606796488199\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0605989398473\n",
      "Truth:  -0.05660377 Pred:  -0.00129086 Error:  -0.0553129084828 Loss:  0.0605916788152\n",
      "Truth:  -0.08490566 Pred:  0.000934591 Error:  -0.0858402505855 Loss:  0.0606263133443\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0605460091147\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0604917357038\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0604118347339\n",
      "Truth:  -0.254717 Pred:  -0.0138851 Error:  -0.240831942469 Loss:  0.0606579740351\n",
      "Truth:  -0.04716982 Pred:  -0.025918 Error:  -0.0212518242789 Loss:  0.0606042871825\n",
      "Truth:  -0.04716982 Pred:  0.00200423 Error:  -0.0491740457383 Loss:  0.0605887358336\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0605091373145\n",
      "Truth:  -0.1509434 Pred:  0.00200423 Error:  -0.152947625738 Loss:  0.0606345626719\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0605806840582\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0605397171186\n",
      "Truth:  -0.3207547 Pred:  -0.0199814 Error:  -0.300773308272 Loss:  0.0608643571067\n",
      "Truth:  -0.03773585 Pred:  0.00200423 Error:  -0.0397400757383 Loss:  0.0608358493046\n",
      "Truth:  -0.08490566 Pred:  0.00200423 Error:  -0.0869098857383 Loss:  0.0608709895154\n",
      "Truth:  -0.03773585 Pred:  0.00200423 Error:  -0.0397400757383 Loss:  0.0608425495238\n",
      "Truth:  0.05660377 Pred:  -0.0165035 Error:  0.0731073089364 Loss:  0.0608590344155\n",
      "Truth:  0.05660377 Pred:  -0.00476367 Error:  0.0613674393344 Loss:  0.0608597168382\n",
      "Truth:  -0.09433963 Pred:  -0.00517924 Error:  -0.0891603896317 Loss:  0.0608976533969\n",
      "Truth:  0.1792453 Pred:  0.00200423 Error:  0.177241074262 Loss:  0.0610534009482\n",
      "Truth:  0.0 Pred:  -0.00100513 Error:  0.00100513058715 Loss:  0.0609731225119\n",
      "Truth:  -0.04716982 Pred:  0.00200423 Error:  -0.0491740457383 Loss:  0.0609573694054\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0608787652139\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0608003703544\n",
      "Truth:  0.09433963 Pred:  0.00200423 Error:  0.0923354042617 Loss:  0.0608423052399\n",
      "Truth:  0.06603774 Pred:  0.00200423 Error:  0.0640335142617 Loss:  0.0608465432333\n",
      "Truth:  -0.1132075 Pred:  -0.000462934 Error:  -0.112744566091 Loss:  0.0609153735023\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0608748314391\n",
      "Truth:  -0.1886793 Pred:  -0.0158018 Error:  -0.17287753125 Loss:  0.0610229831584\n",
      "Truth:  -0.04716982 Pred:  0.00200423 Error:  -0.0491740457383 Loss:  0.0610073306651\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0609543818723\n",
      "Truth:  -0.0754717 Pred:  0.00200423 Error:  -0.0774759257383 Loss:  0.0609761493873\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0608985547509\n",
      "Truth:  -0.009433962 Pred:  0.00200423 Error:  -0.0114381877383 Loss:  0.0608335608389\n",
      "Truth:  -0.1132075 Pred:  0.00169428 Error:  -0.114901777977 Loss:  0.0609045165045\n",
      "Truth:  -0.1981132 Pred:  -0.00123302 Error:  -0.196880178737 Loss:  0.0610827283816\n",
      "Truth:  -0.2735849 Pred:  0.00200423 Error:  -0.275589125738 Loss:  0.0613634959174\n",
      "Truth:  -0.09433963 Pred:  -0.0102202 Error:  -0.0841194488031 Loss:  0.061393242261\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0613403426572\n",
      "Truth:  0.1037736 Pred:  0.00200423 Error:  0.101769374262 Loss:  0.061393053259\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0613157240565\n",
      "Truth:  -0.1886793 Pred:  -0.00267053 Error:  -0.186008772098 Loss:  0.0614778736639\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0614251389523\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0613847770545\n",
      "Truth:  0.0 Pred:  0.000510426 Error:  -0.000510425772518 Loss:  0.0613059242678\n",
      "Truth:  -0.2264151 Pred:  0.00200423 Error:  -0.228419325738 Loss:  0.0615221123681\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0614452158737\n",
      "Truth:  -0.1226415 Pred:  0.00200423 Error:  -0.124645725738 Loss:  0.0615267649187\n",
      "Truth:  -0.04716982 Pred:  -3.50813e-05 Error:  -0.0471347387084 Loss:  0.0615082184932\n",
      "Truth:  0.3113208 Pred:  0.00200423 Error:  0.309316574262 Loss:  0.061827148166\n",
      "Truth:  -0.05660377 Pred:  0.00200423 Error:  -0.0586079957383 Loss:  0.0618230104379\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0617462212406\n",
      "Truth:  -0.05660377 Pred:  -0.000562164 Error:  -0.0560416062317 Loss:  0.0617389076316\n",
      "Truth:  -0.03773585 Pred:  0.00150918 Error:  -0.0392450340178 Loss:  0.061710106257\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0616337560261\n",
      "Truth:  -0.05660377 Pred:  0.000901304 Error:  -0.0575050741854 Loss:  0.0616284831247\n",
      "Truth:  -0.06603774 Pred:  0.000840842 Error:  -0.0668785815597 Loss:  0.0616351796788\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.061583252247\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0615434594525\n",
      "Truth:  -0.1226415 Pred:  -0.0113319 Error:  -0.111309620466 Loss:  0.0616066947269\n",
      "Truth:  0.1792453 Pred:  0.00200423 Error:  0.177241074262 Loss:  0.0617534388634\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0616777110901\n",
      "Truth:  0.0 Pred:  -0.00321569 Error:  0.00321568548679 Loss:  0.0616037085261\n",
      "Truth:  -0.04716982 Pred:  0.00200423 Error:  -0.0491740457383 Loss:  0.0615879946667\n",
      "Truth:  -0.05660377 Pred:  0.00200423 Error:  -0.0586079957383 Loss:  0.0615842320418\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0615447892721\n",
      "Truth:  -0.04716982 Pred:  -0.00154504 Error:  -0.0456247837884 Loss:  0.0615247388874\n",
      "Truth:  -0.1415094 Pred:  0.00200423 Error:  -0.143513625738 Loss:  0.0616278695627\n",
      "Truth:  0.03773585 Pred:  0.00200423 Error:  0.0357316242617 Loss:  0.0615953365912\n",
      "Truth:  -0.05660377 Pred:  -0.0136609 Error:  -0.0429428520101 Loss:  0.0615719332228\n",
      "Truth:  -0.09433963 Pred:  0.00200423 Error:  -0.0963438557383 Loss:  0.0616155070606\n",
      "Truth:  -0.02830189 Pred:  -0.000215067 Error:  -0.0280868231703 Loss:  0.0615735437516\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0615344594666\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0614954727703\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.061421294158\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0613473002995\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0612734905052\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0612350217167\n",
      "Truth:  -0.05660377 Pred:  0.00200423 Error:  -0.0586079957383 Loss:  0.0612317623793\n",
      "Truth:  -0.0754717 Pred:  0.00200423 Error:  -0.0774759257383 Loss:  0.0612518914541\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0611785651351\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0611287426142\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0610557493835\n",
      "Truth:  0.0 Pred:  -0.0110922 Error:  0.011092165485 Loss:  0.0609941420051\n",
      "Truth:  -0.05660377 Pred:  0.00200423 Error:  -0.0586079957383 Loss:  0.0609912034013\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0609418564669\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0608694515152\n",
      "Truth:  -0.0754717 Pred:  0.00200423 Error:  -0.0774759257383 Loss:  0.0608898275572\n",
      "Truth:  0.1509434 Pred:  0.00200423 Error:  0.148939174262 Loss:  0.0609977311683\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0609255236953\n",
      "Truth:  0.08490566 Pred:  0.00200423 Error:  0.0829014342617 Loss:  0.0609523891117\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0609149699745\n",
      "Truth:  -0.2641509 Pred:  -0.000225973 Error:  -0.263924927383 Loss:  0.0611625430932\n",
      "Truth:  -0.06603774 Pred:  0.00142611 Error:  -0.0674638549336 Loss:  0.0611702182599\n",
      "Truth:  -0.06603774 Pred:  0.00200423 Error:  -0.0680419657383 Loss:  0.06117857805\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0611066772574\n",
      "Truth:  0.06603774 Pred:  0.00200423 Error:  0.0640335142617 Loss:  0.0611102292441\n",
      "Truth:  -0.1037736 Pred:  -0.00568905 Error:  -0.0980845499291 Loss:  0.0611550466025\n",
      "Truth:  -0.08490566 Pred:  0.00161475 Error:  -0.0865204124584 Loss:  0.0611857552779\n",
      "Truth:  -0.03773585 Pred:  0.00200423 Error:  -0.0397400757383 Loss:  0.06115982338\n",
      "Truth:  0.1320755 Pred:  0.00200423 Error:  0.130071274262 Loss:  0.06124304977\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0612057313936\n",
      "Truth:  0.0 Pred:  0.000177979 Error:  -0.000177978654392 Loss:  0.0611322039806\n",
      "Truth:  -0.06603774 Pred:  0.00200423 Error:  -0.0680419657383 Loss:  0.0611405189767\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0610694417012\n",
      "Truth:  -0.06603774 Pred:  0.00200423 Error:  -0.0680419657383 Loss:  0.0610778120782\n",
      "Truth:  -0.1698113 Pred:  -0.035562 Error:  -0.134249317458 Loss:  0.0611655476961\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0610946958136\n",
      "Truth:  0.1226415 Pred:  0.00200423 Error:  0.120637274262 Loss:  0.0611659189935\n",
      "Truth:  0.0 Pred:  0.000960367 Error:  -0.000960366800427 Loss:  0.0610939888236\n",
      "Truth:  0.1226415 Pred:  0.00200423 Error:  0.120637274262 Loss:  0.0611650428635\n",
      "Truth:  -0.04716982 Pred:  0.00200423 Error:  -0.0491740457383 Loss:  0.0611507508527\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.061114031049\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0610437459059\n",
      "Truth:  -0.2358491 Pred:  -0.00359124 Error:  -0.232257858685 Loss:  0.0612470880826\n",
      "Truth:  -0.1037736 Pred:  -0.000419461 Error:  -0.103354138599 Loss:  0.0612970371342\n",
      "Truth:  0.08490566 Pred:  0.00200423 Error:  0.0829014342617 Loss:  0.0613226347611\n",
      "Truth:  -0.03773585 Pred:  0.00200423 Error:  -0.0397400757383 Loss:  0.0612970932712\n",
      "Truth:  -0.06603774 Pred:  0.00200423 Error:  -0.0680419657383 Loss:  0.0613050659337\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0612350531353\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0611652054615\n",
      "Truth:  -0.0754717 Pred:  0.00200423 Error:  -0.0774759257383 Loss:  0.0611844171462\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0611147933916\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0610675047339\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0609981816365\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0609290210786\n",
      "Truth:  0.0 Pred:  -0.0313452 Error:  0.03134521842 Loss:  0.0608943796235\n",
      "Truth:  -0.05660377 Pred:  -0.00955903 Error:  -0.0470447393554 Loss:  0.0608781812138\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0608314451911\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0607848182372\n",
      "Truth:  -0.06603774 Pred:  0.00200423 Error:  -0.0680419657383 Loss:  0.0607932764511\n",
      "Truth:  -0.1886793 Pred:  0.000225715 Error:  -0.188905014524 Loss:  0.0609424170076\n",
      "Truth:  -0.09433963 Pred:  0.00130614 Error:  -0.0956457689327 Loss:  0.0609827697424\n",
      "Truth:  -0.08490566 Pred:  0.00200423 Error:  -0.0869098857383 Loss:  0.0610128825369\n",
      "Truth:  -0.04716982 Pred:  0.00200423 Error:  -0.0491740457383 Loss:  0.0609991483874\n",
      "Truth:  -0.1320755 Pred:  -0.00843639 Error:  -0.123639113527 Loss:  0.0610717323563\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0610033671866\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0609351600867\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0608671105089\n",
      "Truth:  -0.04716982 Pred:  0.00200423 Error:  -0.0491740457383 Loss:  0.0608536236983\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0608075620878\n",
      "Truth:  -0.06603774 Pred:  -0.0267561 Error:  -0.039281673171 Loss:  0.0607827912145\n",
      "Truth:  -0.04716982 Pred:  0.00200423 Error:  -0.0491740457383 Loss:  0.0607694478289\n",
      "Truth:  -0.1226415 Pred:  -0.00710038 Error:  -0.115541121837 Loss:  0.0608323314959\n",
      "Truth:  0.0 Pred:  0.00059274 Error:  -0.000592740019783 Loss:  0.0607632493956\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0607283614991\n",
      "Truth:  0.2169811 Pred:  0.00200423 Error:  0.214976874262 Loss:  0.0609048472116\n",
      "Truth:  -0.03773585 Pred:  0.00200423 Error:  -0.0397400757383 Loss:  0.0608806589014\n",
      "Truth:  -0.09433963 Pred:  0.00200423 Error:  -0.0963438557383 Loss:  0.0609211420028\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0608539619386\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0607869349042\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0607200603773\n",
      "Truth:  -0.06603774 Pred:  0.00200423 Error:  -0.0680419657383 Loss:  0.0607283807243\n",
      "Truth:  -0.1320755 Pred:  -0.00871078 Error:  -0.123364717956 Loss:  0.0607994775884\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0607328163051\n",
      "Truth:  -0.04716982 Pred:  0.00200423 Error:  -0.0491740457383 Loss:  0.0607197259647\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0606853214282\n",
      "Truth:  0.06603774 Pred:  0.00200423 Error:  0.0640335142617 Loss:  0.060689104697\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0606228689419\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0605567825347\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0604908449708\n",
      "Truth:  -0.04716982 Pred:  0.00200423 Error:  -0.0491740457383 Loss:  0.0604781151629\n",
      "Truth:  -0.1037736 Pred:  -0.00700135 Error:  -0.0967722502975 Loss:  0.06051889509\n",
      "Truth:  -0.05660377 Pred:  -0.000466689 Error:  -0.0561370805326 Loss:  0.0605139772285\n",
      "Truth:  -0.0754717 Pred:  -0.0294642 Error:  -0.0460075482897 Loss:  0.0604977144158\n",
      "Truth:  -0.0754717 Pred:  0.00200423 Error:  -0.0774759257383 Loss:  0.0605167269705\n",
      "Truth:  -0.1226415 Pred:  0.00200423 Error:  -0.124645725738 Loss:  0.0605884596313\n",
      "Truth:  0.0 Pred:  0.0013336 Error:  -0.0013336034026 Loss:  0.060522253088\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0604569427896\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0603917781107\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0603582751459\n",
      "Truth:  -0.1792453 Pred:  -0.0178622 Error:  -0.161383084362 Loss:  0.0604706497946\n",
      "Truth:  -0.03773585 Pred:  0.00200423 Error:  -0.0397400757383 Loss:  0.0604476158234\n",
      "Truth:  -0.009433962 Pred:  0.00200423 Error:  -0.0114381877383 Loss:  0.0603932213417\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.060349406402\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0603161358697\n",
      "Truth:  -0.05660377 Pred:  0.00200423 Error:  -0.0586079957383 Loss:  0.0603142463341\n",
      "Truth:  -0.1981132 Pred:  -0.00453122 Error:  -0.193581977604 Loss:  0.0604615034957\n",
      "Truth:  -0.1132075 Pred:  -0.000166865 Error:  -0.113040635167 Loss:  0.0605195378574\n",
      "Truth:  -0.1509434 Pred:  -0.00932515 Error:  -0.141618254256 Loss:  0.0606089520982\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.060544409448\n",
      "Truth:  -0.05660377 Pred:  0.00200423 Error:  -0.0586079957383 Loss:  0.0605422791799\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0604779516487\n",
      "Truth:  -0.04716982 Pred:  0.00200423 Error:  -0.0491740457383 Loss:  0.0604655434095\n",
      "Truth:  -0.1792453 Pred:  -0.0223679 Error:  -0.156877375548 Loss:  0.0605712581377\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.060507110238\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0604431027057\n",
      "Truth:  0.009433962 Pred:  0.00200423 Error:  0.00742973626165 Loss:  0.0603851646003\n",
      "Truth:  0.2735849 Pred:  0.00200423 Error:  0.271580674262 Loss:  0.0606157273838\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0605723865096\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.06052914006\n",
      "Truth:  0.3113208 Pred:  0.00200423 Error:  0.309316574262 Loss:  0.0607998554399\n",
      "Truth:  -0.05660377 Pred:  0.00200423 Error:  -0.0586079957383 Loss:  0.0607974729838\n",
      "Truth:  -0.06603774 Pred:  0.00129336 Error:  -0.0673311049123 Loss:  0.0608045670466\n",
      "Truth:  -0.08490566 Pred:  -0.00187005 Error:  -0.0830356145616 Loss:  0.0608286788118\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.06076494701\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0607013531558\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0606684934397\n",
      "Truth:  0.0 Pred:  0.000475722 Error:  -0.00047572189942 Loss:  0.0606034904467\n",
      "Truth:  -0.009433962 Pred:  0.00200423 Error:  -0.0114381877383 Loss:  0.0605504534427\n",
      "Truth:  -0.03773585 Pred:  0.00200423 Error:  -0.0397400757383 Loss:  0.0605280284667\n",
      "Truth:  -0.06603774 Pred:  0.00200423 Error:  -0.0680419657383 Loss:  0.0605361166662\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.060473179149\n",
      "Truth:  -0.2075472 Pred:  -0.0458692 Error:  -0.161677976227 Loss:  0.0605818846239\n",
      "Truth:  -0.03773585 Pred:  0.00200423 Error:  -0.0397400757383 Loss:  0.060559522168\n",
      "Truth:  -0.04716982 Pred:  0.00200423 Error:  -0.0491740457383 Loss:  0.060547319085\n",
      "Truth:  -0.1792453 Pred:  -0.0201736 Error:  -0.159071687018 Loss:  0.0606528055603\n",
      "Truth:  -0.05660377 Pred:  0.00200423 Error:  -0.0586079957383 Loss:  0.0606506185979\n",
      "Truth:  -0.09433963 Pred:  -0.0094644 Error:  -0.0848752329049 Loss:  0.0606764995961\n",
      "Truth:  0.5188679 Pred:  0.00200423 Error:  0.516863674262 Loss:  0.0611633589074\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.061100289469\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0610373543639\n",
      "Truth:  -0.1792453 Pred:  -0.00812398 Error:  -0.171121317302 Loss:  0.0611544649628\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0610916060475\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0610489102298\n",
      "Truth:  -0.009433962 Pred:  0.00200423 Error:  -0.0114381877383 Loss:  0.060996300768\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0609338091631\n",
      "Truth:  -0.1320755 Pred:  -0.0115762 Error:  -0.120499265637 Loss:  0.0609968413922\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0609344813333\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0608722529747\n",
      "Truth:  0.009433962 Pred:  0.00200423 Error:  0.00742973626165 Loss:  0.0608158790119\n",
      "Truth:  -0.1226415 Pred:  -0.0305496 Error:  -0.0920919290819 Loss:  0.0608488358613\n",
      "Truth:  -0.1132075 Pred:  0.00200423 Error:  -0.115211725738 Loss:  0.0609060599559\n",
      "Truth:  -0.04716982 Pred:  0.00200423 Error:  -0.0491740457383 Loss:  0.0608937234531\n",
      "Truth:  -0.03773585 Pred:  0.00200423 Error:  -0.0397400757383 Loss:  0.0608715032349\n",
      "Truth:  -0.1509434 Pred:  0.000666841 Error:  -0.151610240817 Loss:  0.0609667170204\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.060904911474\n",
      "Truth:  0.0 Pred:  9.13728e-05 Error:  -9.13727562875e-05 Loss:  0.0608412323758\n",
      "Truth:  -0.09433963 Pred:  -0.00202246 Error:  -0.0923171726909 Loss:  0.0608741569996\n",
      "Truth:  0.08490566 Pred:  0.00200423 Error:  0.0829014342617 Loss:  0.0608971740082\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0608356991144\n",
      "Truth:  0.03773585 Pred:  0.00200423 Error:  0.0357316242617 Loss:  0.0608095217684\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.060767920335\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0607362223074\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0606751703359\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0606338380154\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0606023777226\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.060541654249\n",
      "Truth:  0.03773585 Pred:  0.00200423 Error:  0.0357316242617 Loss:  0.0605159709882\n",
      "Truth:  -0.1037736 Pred:  0.000879177 Error:  -0.10465277666 Loss:  0.0605616140137\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.060501120844\n",
      "Truth:  -0.0754717 Pred:  0.00200423 Error:  -0.0774759257383 Loss:  0.0605186387025\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0604583145654\n",
      "Truth:  -0.04716982 Pred:  0.00200423 Error:  -0.0491740457383 Loss:  0.0604466932793\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0604059787242\n",
      "Truth:  -0.09433963 Pred:  -0.00172153 Error:  -0.0926181009428 Loss:  0.060439084708\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0603790899863\n",
      "Truth:  -0.0754717 Pred:  -0.00410046 Error:  -0.0713712399065 Loss:  0.0603903639862\n",
      "Truth:  -0.1886793 Pred:  0.00200423 Error:  -0.190683525738 Loss:  0.0605238610781\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0604639638055\n",
      "Truth:  -0.2075472 Pred:  -0.00264041 Error:  -0.204906789674 Loss:  0.0606116558565\n",
      "Truth:  0.0 Pred:  -0.00496722 Error:  0.00496721593663 Loss:  0.0605548178178\n",
      "Truth:  -0.1132075 Pred:  0.00200423 Error:  -0.115211725738 Loss:  0.0606105901728\n",
      "Truth:  -0.1037736 Pred:  -0.0071327 Error:  -0.0966409030809 Loss:  0.0606473183205\n",
      "Truth:  0.03773585 Pred:  0.00200423 Error:  0.0357316242617 Loss:  0.0606219459233\n",
      "Truth:  0.0 Pred:  -0.00366446 Error:  0.00366445677355 Loss:  0.0605640034115\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0605044914423\n",
      "Truth:  -0.04716982 Pred:  0.0019803 Error:  -0.0491501156376 Loss:  0.0604929641572\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0604527807714\n",
      "Truth:  -0.05660377 Pred:  -0.0043306 Error:  -0.0522731689225 Loss:  0.060444493424\n",
      "Truth:  -0.06603774 Pred:  -0.00109617 Error:  -0.0649415677417 Loss:  0.0604490451187\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0603899502558\n",
      "Truth:  -0.0754717 Pred:  -0.000951632 Error:  -0.0745200676083 Loss:  0.0604042231016\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0603452927309\n",
      "Truth:  -0.05660377 Pred:  3.5861e-05 Error:  -0.0566396310414 Loss:  0.0603415571848\n",
      "Truth:  0.1886793 Pred:  0.00200423 Error:  0.186675074262 Loss:  0.0604687812705\n",
      "Truth:  -0.1037736 Pred:  0.00130807 Error:  -0.10508167329 Loss:  0.0605136634557\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0604548600007\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0603961746249\n",
      "Truth:  -0.009433962 Pred:  0.00200423 Error:  -0.0114381877383 Loss:  0.0603470693221\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0603169681662\n",
      "Truth:  -0.04716982 Pred:  0.00200423 Error:  -0.0491740457383 Loss:  0.0603058140897\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0602663804213\n",
      "Truth:  -0.009433962 Pred:  0.00200423 Error:  -0.0114381877383 Loss:  0.0602176010081\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0601877492264\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0601297397314\n",
      "Truth:  0.1037736 Pred:  0.00200423 Error:  0.101769374262 Loss:  0.0601712134709\n",
      "Truth:  -0.1415094 Pred:  0.00110853 Error:  -0.14261793347 Loss:  0.0602532500083\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0601953483937\n",
      "Truth:  -0.06603774 Pred:  0.00106051 Error:  -0.0670982513348 Loss:  0.0602022033122\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0601444672233\n",
      "Truth:  0.0 Pred:  0.000940775 Error:  -0.000940775149502 Loss:  0.0600857916117\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0600282851109\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0599988863281\n",
      "Truth:  0.0 Pred:  -0.000936985 Error:  0.000936984783038 Loss:  0.0599405247653\n",
      "Truth:  -0.009433962 Pred:  0.00200423 Error:  -0.0114381877383 Loss:  0.059892644867\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0598355556962\n",
      "Truth:  -0.009433962 Pred:  0.00200423 Error:  -0.0114381877383 Loss:  0.0597878735603\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0597309998912\n",
      "Truth:  0.01886792 Pred:  0.00200423 Error:  0.0168636942617 Loss:  0.0596888491482\n",
      "Truth:  -0.1132075 Pred:  -0.00172228 Error:  -0.111485219598 Loss:  0.0597397296693\n",
      "Truth:  -0.1037736 Pred:  0.00125324 Error:  -0.10502683562 Loss:  0.059784172364\n",
      "Truth:  0.009433962 Pred:  0.00200423 Error:  0.00742973626165 Loss:  0.0597328444855\n",
      "Truth:  -0.08490566 Pred:  0.00200423 Error:  -0.0869098857383 Loss:  0.0597594625475\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0597214123353\n",
      "Truth:  0.1509434 Pred:  0.00200423 Error:  0.148939174262 Loss:  0.0598086242238\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0597521746159\n",
      "Truth:  0.04716982 Pred:  -0.00525242 Error:  0.0524222446275 Loss:  0.0597450234647\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0597071356696\n",
      "Truth:  0.3113208 Pred:  0.00200423 Error:  0.309316574262 Loss:  0.0599501828348\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0598938151722\n",
      "Truth:  -0.05660377 Pred:  0.00200423 Error:  -0.0586079957383 Loss:  0.0598925655907\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.059836363319\n",
      "Truth:  -0.08490566 Pred:  -0.00276312 Error:  -0.0821425423364 Loss:  0.0598579987981\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0598202218087\n",
      "Truth:  -0.03773585 Pred:  0.00200423 Error:  -0.0397400757383 Loss:  0.0598007831388\n",
      "Truth:  -0.1226415 Pred:  -0.0164769 Error:  -0.106164615516 Loss:  0.0598456224351\n",
      "Truth:  -0.06603774 Pred:  0.000357846 Error:  -0.0663955863802 Loss:  0.0598519509026\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0598234317567\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0597676755311\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0597120267355\n",
      "Truth:  -0.05660377 Pred:  0.00200423 Error:  -0.0586079957383 Loss:  0.0597109641455\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0596826902528\n",
      "Truth:  -0.03773585 Pred:  0.00200423 Error:  -0.0397400757383 Loss:  0.0596635330823\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0596081978545\n",
      "Truth:  -0.05660377 Pred:  0.00200423 Error:  -0.0586079957383 Loss:  0.059607238888\n",
      "Truth:  -0.1792453 Pred:  -0.0082196 Error:  -0.171025701208 Loss:  0.059713961553\n",
      "Truth:  -0.04716982 Pred:  0.00200423 Error:  -0.0491740457383 Loss:  0.0597038755092\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0596487133201\n",
      "Truth:  -0.0754717 Pred:  0.00200423 Error:  -0.0774759257383 Loss:  0.0596657402661\n",
      "Truth:  -0.04716982 Pred:  0.00200423 Error:  -0.0491740457383 Loss:  0.0596557291072\n",
      "Truth:  -0.1320755 Pred:  -0.00808947 Error:  -0.12398602746 Loss:  0.0597170544631\n",
      "Truth:  -0.1320755 Pred:  -0.00934312 Error:  -0.122732380662 Loss:  0.0597770690595\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0597220996557\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0596672347566\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0596124740642\n",
      "Truth:  0.05660377 Pred:  -0.00581486 Error:  0.0624186259183 Loss:  0.0596151364474\n",
      "Truth:  0.2641509 Pred:  0.00200423 Error:  0.262146674262 Loss:  0.059807109469\n",
      "Truth:  -0.06603774 Pred:  0.00200423 Error:  -0.0680419657383 Loss:  0.0598149076284\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0597780649019\n",
      "Truth:  0.08490566 Pred:  0.00200423 Error:  0.0829014342617 Loss:  0.0597999206385\n",
      "Truth:  -0.06603774 Pred:  0.00200423 Error:  -0.0680419657383 Loss:  0.0598077034951\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0597531719123\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0597254178537\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0596710664487\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0596168173041\n",
      "Truth:  -0.1981132 Pred:  -0.029428 Error:  -0.168685220859 Loss:  0.0597193252022\n",
      "Truth:  -0.03773585 Pred:  0.00200423 Error:  -0.0397400757383 Loss:  0.0597005653435\n",
      "Truth:  -0.1698113 Pred:  -0.013989 Error:  -0.155822319908 Loss:  0.059790735845\n",
      "Truth:  -0.08490566 Pred:  0.00200423 Error:  -0.0869098857383 Loss:  0.0598161521054\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0597620210882\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0597079913452\n",
      "Truth:  0.5188679 Pred:  0.00200423 Error:  0.516863674262 Loss:  0.0601352396469\n",
      "Truth:  -0.0754717 Pred:  0.00200423 Error:  -0.0774759257383 Loss:  0.0601514307637\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0601147896397\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0600782168122\n",
      "Truth:  -0.1037736 Pred:  4.26329e-05 Error:  -0.103816232921 Loss:  0.0601189412219\n",
      "Truth:  -0.01886792 Pred:  -0.0470442 Error:  0.0281762752348 Loss:  0.060089227114\n",
      "Truth:  -0.1320755 Pred:  -0.00172289 Error:  -0.130352612609 Loss:  0.0601545276582\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0601005348059\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0600466421259\n",
      "Truth:  -0.1132075 Pred:  0.00200423 Error:  -0.115211725738 Loss:  0.060097768246\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0600439779289\n",
      "Truth:  -0.08490566 Pred:  0.00200423 Error:  -0.0869098857383 Loss:  0.0600688307575\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0600326046161\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0599790234722\n",
      "Truth:  -0.1226415 Pred:  -0.0188461 Error:  -0.103795412842 Loss:  0.0600194444956\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0599833640359\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0599299762474\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0598766866885\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0598495078549\n",
      "Truth:  -0.08490566 Pred:  0.00200423 Error:  -0.0869098857383 Loss:  0.0598743566868\n",
      "Truth:  0.05660377 Pred:  0.00200423 Error:  0.0545995442617 Loss:  0.0598695174093\n",
      "Truth:  0.009433962 Pred:  -0.0223869 Error:  0.0318208221024 Loss:  0.0598438082477\n",
      "Truth:  -0.0754717 Pred:  0.00200423 Error:  -0.0774759257383 Loss:  0.0598599548755\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0598070219119\n",
      "Truth:  -0.06603774 Pred:  -0.00819339 Error:  -0.0578443514187 Loss:  0.0598052278804\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0597524415771\n",
      "Truth:  -0.04716982 Pred:  0.00200423 Error:  -0.0491740457383 Loss:  0.0597427897561\n",
      "Truth:  -0.05660377 Pred:  -0.000450663 Error:  -0.0561531067314 Loss:  0.0597395174835\n",
      "Truth:  -0.03773585 Pred:  0.00194531 Error:  -0.039681157944 Loss:  0.0597212493965\n",
      "Truth:  -0.1132075 Pred:  -0.00383899 Error:  -0.109368512445 Loss:  0.0597664243401\n",
      "Truth:  -0.03773585 Pred:  0.00200423 Error:  -0.0397400757383 Loss:  0.0597482185686\n",
      "Truth:  -0.1886793 Pred:  -5.54977e-05 Error:  -0.188623802255 Loss:  0.0598652717782\n",
      "Truth:  -0.04716982 Pred:  0.00200423 Error:  -0.0491740457383 Loss:  0.0598555701211\n",
      "Truth:  0.03773585 Pred:  0.00200423 Error:  0.0357316242617 Loss:  0.05983369891\n",
      "Truth:  -0.08490566 Pred:  0.00200423 Error:  -0.0869098857383 Loss:  0.0598582244416\n",
      "Truth:  -0.2830189 Pred:  -0.0268808 Error:  -0.256138054572 Loss:  0.0600358532471\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0599833834212\n",
      "Truth:  -0.0754717 Pred:  -0.00110099 Error:  -0.0743707093116 Loss:  0.0599963801022\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0599695838347\n",
      "Truth:  0.254717 Pred:  0.00200423 Error:  0.252712774262 Loss:  0.0601433829244\n",
      "Truth:  -0.05660377 Pred:  0.00200423 Error:  -0.0586079957383 Loss:  0.0601419996927\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0600896704632\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.060037435351\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0600022464116\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.059967120648\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0599405188498\n",
      "Truth:  -0.04716982 Pred:  0.00200423 Error:  -0.0491740457383 Loss:  0.0599308714725\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0599043497574\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0598525607377\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0598008642811\n",
      "Truth:  -0.04716982 Pred:  0.00200423 Error:  -0.0491740457383 Loss:  0.0597913760502\n",
      "Truth:  -0.0754717 Pred:  -0.0126558 Error:  -0.0628158914096 Loss:  0.0597940741014\n",
      "Truth:  -0.2169811 Pred:  -0.0101491 Error:  -0.206831956364 Loss:  0.0599251239073\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0598735469722\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0598472414284\n",
      "Truth:  -0.1509434 Pred:  -0.00940319 Error:  -0.141540205699 Loss:  0.0599198573966\n",
      "Truth:  -0.2075472 Pred:  0.00200423 Error:  -0.209551425738 Loss:  0.0600527451127\n",
      "Truth:  -0.0754717 Pred:  -0.000572278 Error:  -0.0748994216028 Loss:  0.0600659187387\n",
      "Truth:  -0.1037736 Pred:  0.00200423 Error:  -0.105777825738 Loss:  0.060106443479\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0600549800443\n",
      "Truth:  -0.09433963 Pred:  0.00200423 Error:  -0.0963438557383 Loss:  0.0600870940936\n",
      "Truth:  -0.01886792 Pred:  -0.00130167 Error:  -0.0175662497858 Loss:  0.0600494982984\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0600232232255\n",
      "Truth:  -0.04716982 Pred:  0.00200423 Error:  -0.0491740457383 Loss:  0.0600136476055\n",
      "Truth:  -0.03773585 Pred:  -0.0170958 Error:  -0.0206400234338 Loss:  0.0599789265965\n",
      "Truth:  -0.09433963 Pred:  0.000915809 Error:  -0.0952554391852 Loss:  0.0600100072243\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0599755548815\n",
      "Truth:  -0.254717 Pred:  -0.0323817 Error:  -0.222335297786 Loss:  0.0601183514891\n",
      "Truth:  -0.05660377 Pred:  -1.48758e-05 Error:  -0.0565888942173 Loss:  0.0601152500328\n",
      "Truth:  -0.02830189 Pred:  -0.00363716 Error:  -0.0246647284285 Loss:  0.0600841257821\n",
      "Truth:  -0.09433963 Pred:  -0.0109586 Error:  -0.0833810664636 Loss:  0.060104561695\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0600536411551\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0600028097931\n",
      "Truth:  -0.1509434 Pred:  0.00200423 Error:  -0.152947625738 Loss:  0.0600841263425\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0600498501357\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0599991552672\n",
      "Truth:  -0.04716982 Pred:  0.00200423 Error:  -0.0491740457383 Loss:  0.0599897092729\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0599638299411\n",
      "Truth:  0.0 Pred:  -0.0300584 Error:  0.030058439821 Loss:  0.0599377799497\n",
      "Truth:  -0.04716982 Pred:  0.00200423 Error:  -0.0491740457383 Loss:  0.0599284120348\n",
      "Truth:  -0.0754717 Pred:  -0.000716621 Error:  -0.0747550789438 Loss:  0.0599413047886\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0598909684906\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.059840719582\n",
      "Truth:  -0.05660377 Pred:  0.00200423 Error:  -0.0586079957383 Loss:  0.0598396504373\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0597895330848\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0597395025157\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0596895585046\n",
      "Truth:  -0.03773585 Pred:  0.00200423 Error:  -0.0397400757383 Loss:  0.0596723160822\n",
      "Truth:  0.1603774 Pred:  0.00200423 Error:  0.158373174262 Loss:  0.0597575499839\n",
      "Truth:  -0.06603774 Pred:  0.00200423 Error:  -0.0680419657383 Loss:  0.0597646978836\n",
      "Truth:  -0.04716982 Pred:  0.00200423 Error:  -0.0491740457383 Loss:  0.0597555680111\n",
      "Truth:  -0.0754717 Pred:  -0.000634739 Error:  -0.0748369612902 Loss:  0.0597685580139\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0597188468846\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.059669221243\n",
      "Truth:  -0.04716982 Pred:  6.80994e-05 Error:  -0.0472379193544 Loss:  0.0596585414304\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0596090527474\n",
      "Truth:  0.0754717 Pred:  0.00200423 Error:  0.0734674742617 Loss:  0.0596209381861\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0595958183725\n",
      "Truth:  0.03773585 Pred:  0.00200423 Error:  0.0357316242617 Loss:  0.0595753866995\n",
      "Truth:  -0.1415094 Pred:  0.00200423 Error:  -0.143513625738 Loss:  0.0596471901546\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.059614049091\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0595809646304\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0595318377201\n",
      "Truth:  -0.09433963 Pred:  -0.00143015 Error:  -0.0929094810427 Loss:  0.059560292659\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0595112670483\n",
      "Truth:  -0.03773585 Pred:  0.00200423 Error:  -0.0397400757383 Loss:  0.0594944405025\n",
      "Truth:  -0.1226415 Pred:  -0.00715009 Error:  -0.115491412494 Loss:  0.0595420569753\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0594931718171\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0594443696557\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.05939565028\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0593630032423\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0593144352681\n",
      "Truth:  -0.04716982 Pred:  0.00200423 Error:  -0.0491740457383 Loss:  0.0593058562583\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0592574187008\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0592090629635\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0591607888393\n",
      "Truth:  -0.06603774 Pred:  -0.00106557 Error:  -0.064972174494 Loss:  0.0591656888272\n",
      "Truth:  -0.009433962 Pred:  0.00200423 Error:  -0.0114381877383 Loss:  0.0591254803174\n",
      "Truth:  -0.05660377 Pred:  0.00200423 Error:  -0.0586079957383 Loss:  0.0591250447244\n",
      "Truth:  -0.0754717 Pred:  0.00200423 Error:  -0.0774759257383 Loss:  0.0591404786024\n",
      "Truth:  -0.0754717 Pred:  0.00200423 Error:  -0.0774759257383 Loss:  0.0591558865412\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0591079002601\n",
      "Truth:  -0.04716982 Pred:  0.00200423 Error:  -0.0491740457383 Loss:  0.0590995664895\n",
      "Truth:  -0.04716982 Pred:  0.00200423 Error:  -0.0491740457383 Loss:  0.0590912466901\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0590592373928\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0590351762031\n",
      "Truth:  0.1509434 Pred:  0.00200423 Error:  0.148939174262 Loss:  0.05911034677\n",
      "Truth:  0.1037736 Pred:  0.00200423 Error:  0.101769374262 Loss:  0.0591459850553\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0590982874265\n",
      "Truth:  -0.05660377 Pred:  0.00200423 Error:  -0.0586079957383 Loss:  0.0590978785094\n",
      "Truth:  0.3113208 Pred:  0.00200423 Error:  0.309316574262 Loss:  0.0593063940891\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0592586820422\n",
      "Truth:  0.4811321 Pred:  0.00200423 Error:  0.479127874262 Loss:  0.0596079908544\n",
      "Truth:  -0.2075472 Pred:  0.00200423 Error:  -0.209551425738 Loss:  0.0597326321136\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0596846849323\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0596368173313\n",
      "Truth:  -0.1226415 Pred:  -0.00173173 Error:  -0.120909769002 Loss:  0.0596876240905\n",
      "Truth:  -0.1320755 Pred:  -0.0305843 Error:  -0.101491179574 Loss:  0.0597222583536\n",
      "Truth:  -0.03773585 Pred:  0.00200423 Error:  -0.0397400757383 Loss:  0.0597057168117\n",
      "Truth:  -0.009433962 Pred:  0.00200423 Error:  -0.0114381877383 Loss:  0.0596657932971\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0596181391091\n",
      "Truth:  -0.04716982 Pred:  0.00200423 Error:  -0.0491740457383 Loss:  0.0596095147545\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0595853370326\n",
      "Truth:  0.1603774 Pred:  0.00200423 Error:  0.158373174262 Loss:  0.0596667779536\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0596348219139\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0596029184767\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0595555511307\n",
      "Truth:  -0.09433963 Pred:  0.00200423 Error:  -0.0963438557383 Loss:  0.0595857798116\n",
      "Truth:  -0.009433962 Pred:  0.00200423 Error:  -0.0114381877383 Loss:  0.0595462497688\n",
      "Truth:  -0.0754717 Pred:  0.00200423 Error:  -0.0774759257383 Loss:  0.0595609582807\n",
      "Truth:  -0.09433963 Pred:  -0.0126739 Error:  -0.0816657612023 Loss:  0.0595790769716\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0595473759632\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0595002866423\n",
      "Truth:  -0.1132075 Pred:  -0.0554555 Error:  -0.0577520276797 Loss:  0.0594988571582\n",
      "Truth:  -0.0754717 Pred:  0.00200423 Error:  -0.0774759257383 Loss:  0.0595135443058\n",
      "Truth:  -0.05660377 Pred:  0.00200423 Error:  -0.0586079957383 Loss:  0.0595128050824\n",
      "Truth:  -0.08490566 Pred:  0.000800343 Error:  -0.0857060028811 Loss:  0.0595341698441\n",
      "Truth:  -0.1037736 Pred:  0.000621285 Error:  -0.104394885406 Loss:  0.0595707311445\n",
      "Truth:  0.1132075 Pred:  0.00200423 Error:  0.111203274262 Loss:  0.0596127771894\n",
      "Truth:  0.08490566 Pred:  0.00200423 Error:  0.0829014342617 Loss:  0.0596317264628\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0596002146086\n",
      "Truth:  -0.1037736 Pred:  0.00200423 Error:  -0.105777825738 Loss:  0.0596377268841\n",
      "Truth:  0.0 Pred:  -0.0412137 Error:  0.0412137433887 Loss:  0.059622772352\n",
      "Truth:  -0.03773585 Pred:  3.78024e-05 Error:  -0.0377736523833 Loss:  0.0596050520601\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0595583739188\n",
      "Truth:  -0.06603774 Pred:  0.00119412 Error:  -0.0672318552625 Loss:  0.059564587264\n",
      "Truth:  -0.1320755 Pred:  -0.0112314 Error:  -0.120844141837 Loss:  0.0596141661917\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0595904733457\n",
      "Truth:  0.0 Pred:  -0.00155775 Error:  0.00155774573795 Loss:  0.0595435971522\n",
      "Truth:  -0.06603774 Pred:  -0.000135694 Error:  -0.0659020456024 Loss:  0.0595487290718\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0595251463191\n",
      "Truth:  -0.2075472 Pred:  0.000151402 Error:  -0.207698601851 Loss:  0.0596445447523\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0595981354777\n",
      "Truth:  -0.009433962 Pred:  0.00200423 Error:  -0.0114381877383 Loss:  0.0595593905479\n",
      "Truth:  0.0 Pred:  -0.0294235 Error:  0.0294235236943 Loss:  0.0595351655745\n",
      "Truth:  0.0 Pred:  -0.000582646 Error:  0.000582646112889 Loss:  0.0594878141533\n",
      "Truth:  -0.009433962 Pred:  0.00200423 Error:  -0.0114381877383 Loss:  0.0594492510502\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0594031844702\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0593571917148\n",
      "Truth:  -0.03773585 Pred:  0.00200423 Error:  -0.0397400757383 Loss:  0.059341485457\n",
      "Truth:  0.0754717 Pred:  0.00200423 Error:  0.0734674742617 Loss:  0.059352786248\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0593069440734\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.059261175129\n",
      "Truth:  -0.1886793 Pred:  0.000881228 Error:  -0.189560527781 Loss:  0.0593651650354\n",
      "Truth:  -0.1037736 Pred:  -0.00621207 Error:  -0.0975615294157 Loss:  0.0593956246561\n",
      "Truth:  -0.03773585 Pred:  0.00200423 Error:  -0.0397400757383 Loss:  0.0593799628641\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0593342815447\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0593036831869\n",
      "Truth:  0.08490566 Pred:  0.00200423 Error:  0.0829014342617 Loss:  0.0593224413356\n",
      "Truth:  -0.1226415 Pred:  -0.0522912 Error:  -0.0703502780833 Loss:  0.0593312005388\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0592857029397\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0592552401663\n",
      "Truth:  -0.09433963 Pred:  0.00200423 Error:  -0.0963438557383 Loss:  0.0592846289267\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0592542152425\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0592238496812\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0591786167769\n",
      "Truth:  -0.1226415 Pred:  -0.00498718 Error:  -0.117654324424 Loss:  0.0592248061194\n",
      "Truth:  -0.1320755 Pred:  -0.00788543 Error:  -0.124190072786 Loss:  0.0592760809944\n",
      "Truth:  -0.1603774 Pred:  -0.0154028 Error:  -0.144974605184 Loss:  0.0593436665813\n",
      "Truth:  -0.06603774 Pred:  -0.00839517 Error:  -0.0576425738637 Loss:  0.0593423260827\n",
      "Truth:  -0.0754717 Pred:  0.00200423 Error:  -0.0774759257383 Loss:  0.0593566045076\n",
      "Truth:  -0.0754717 Pred:  0.00200423 Error:  -0.0774759257383 Loss:  0.0593708604646\n",
      "Truth:  -0.04716982 Pred:  0.00200423 Error:  -0.0491740457383 Loss:  0.0593628441008\n",
      "Truth:  -0.06603774 Pred:  0.00200423 Error:  -0.0680419657383 Loss:  0.0593696619497\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0593246341347\n",
      "Truth:  -0.09433963 Pred:  -0.0202814 Error:  -0.0740582220179 Loss:  0.0593361898899\n",
      "Truth:  -0.1320755 Pred:  -0.0232817 Error:  -0.108793750662 Loss:  0.0593749497338\n",
      "Truth:  -0.05660377 Pred:  0.00200423 Error:  -0.0586079957383 Loss:  0.0593743491434\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0593516040468\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0593067663781\n",
      "Truth:  -0.04716982 Pred:  0.00200423 Error:  -0.0491740457383 Loss:  0.0592988501901\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0592541237073\n",
      "Truth:  -0.08490566 Pred:  0.00200423 Error:  -0.0869098857383 Loss:  0.0592756960646\n",
      "Truth:  -0.254717 Pred:  0.00200423 Error:  -0.256721225738 Loss:  0.0594295896964\n",
      "Truth:  -0.05660377 Pred:  0.00200423 Error:  -0.0586079957383 Loss:  0.0594289498257\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0593842613245\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0593616500138\n",
      "Truth:  0.3113208 Pred:  0.00200423 Error:  0.309316574262 Loss:  0.0595558651841\n",
      "Truth:  -0.03773585 Pred:  0.00200423 Error:  -0.0397400757383 Loss:  0.0595404802544\n",
      "Truth:  -0.06603774 Pred:  0.000820074 Error:  -0.0668578138812 Loss:  0.0595461570067\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.059516177153\n",
      "Truth:  -0.08490566 Pred:  0.00200423 Error:  -0.0869098857383 Loss:  0.0595373961372\n",
      "Truth:  -0.1603774 Pred:  -0.00569747 Error:  -0.154679933101 Loss:  0.0596110358717\n",
      "Truth:  0.0 Pred:  0.00181226 Error:  -0.00181226199493 Loss:  0.0595663345771\n",
      "Truth:  0.0 Pred:  -0.0132766 Error:  0.0132765537128 Loss:  0.0595305619489\n",
      "Truth:  0.1509434 Pred:  0.00200423 Error:  0.148939174262 Loss:  0.0595996033484\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0595697210508\n",
      "Truth:  0.0 Pred:  -0.00101262 Error:  0.00101261562668 Loss:  0.0595245729356\n",
      "Truth:  -0.04716982 Pred:  0.00200423 Error:  -0.0491740457383 Loss:  0.0595165987236\n",
      "Truth:  -0.04716982 Pred:  -0.00127303 Error:  -0.0458967912146 Loss:  0.0595061138834\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0594836523463\n",
      "Truth:  -0.05660377 Pred:  0.00200423 Error:  -0.0586079957383 Loss:  0.0594829792821\n",
      "Truth:  -0.05660377 Pred:  0.0015468 Error:  -0.0581505728141 Loss:  0.0594819559284\n",
      "Truth:  -0.1886793 Pred:  -0.0309726 Error:  -0.157706696291 Loss:  0.059557339459\n",
      "Truth:  -0.2264151 Pred:  -0.0464504 Error:  -0.179964719764 Loss:  0.0596496764071\n",
      "Truth:  -0.06603774 Pred:  -0.00328811 Error:  -0.0627496330072 Loss:  0.0596520518527\n",
      "Truth:  -0.1037736 Pred:  -0.000472324 Error:  -0.103301276264 Loss:  0.0596854739235\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0596557774214\n",
      "Truth:  0.03773585 Pred:  0.00200423 Error:  0.0357316242617 Loss:  0.0596374867845\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0596078723146\n",
      "Truth:  -0.05660377 Pred:  0.00200423 Error:  -0.0586079957383 Loss:  0.0596071090501\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.059563170924\n",
      "Truth:  -0.06603774 Pred:  -0.00819766 Error:  -0.0578400813047 Loss:  0.0595618575935\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0595180208594\n",
      "Truth:  -0.1792453 Pred:  -0.0210749 Error:  -0.158170394008 Loss:  0.0595930987689\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0595493049491\n",
      "Truth:  -0.09433963 Pred:  0.0016232 Error:  -0.0959628279243 Loss:  0.0595769747994\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.059533259728\n",
      "Truth:  -0.08490566 Pred:  0.000595308 Error:  -0.0855009675597 Loss:  0.059552962086\n",
      "Truth:  0.08490566 Pred:  0.00200423 Error:  0.0829014342617 Loss:  0.0595706637328\n",
      "Truth:  -0.04716982 Pred:  -0.0398694 Error:  -0.00730042584664 Loss:  0.0595310650677\n",
      "Truth:  0.0 Pred:  -0.00509005 Error:  0.00509004713967 Loss:  0.0594898530935\n",
      "Truth:  0.1509434 Pred:  0.00200423 Error:  0.148939174262 Loss:  0.0595575152124\n",
      "Truth:  0.1603774 Pred:  0.00200423 Error:  0.158373174262 Loss:  0.0596322058088\n",
      "Truth:  0.1603774 Pred:  0.00200423 Error:  0.158373174262 Loss:  0.0597067835796\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0596632344793\n",
      "Truth:  -0.2075472 Pred:  -0.0184806 Error:  -0.189066601073 Loss:  0.0597608237452\n",
      "Truth:  0.3113208 Pred:  0.00200423 Error:  0.309316574262 Loss:  0.0599488838436\n",
      "Truth:  -0.04716982 Pred:  -0.000387085 Error:  -0.046782734864 Loss:  0.0599389695748\n",
      "Truth:  -0.05660377 Pred:  0.00200423 Error:  -0.0586079957383 Loss:  0.0599379680896\n",
      "Truth:  -0.1132075 Pred:  0.00124415 Error:  -0.114451645911 Loss:  0.0599789558172\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0599353985445\n",
      "Truth:  -0.03773585 Pred:  0.00200423 Error:  -0.0397400757383 Loss:  0.0599202368907\n",
      "Truth:  -0.1509434 Pred:  0.00106176 Error:  -0.15200516408 Loss:  0.0599893178563\n",
      "Truth:  -0.1132075 Pred:  0.00200423 Error:  -0.115211725738 Loss:  0.0600307139642\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0600084483475\n",
      "Truth:  -0.02830189 Pred:  0.000107617 Error:  -0.0284095066518 Loss:  0.0599847964451\n",
      "Truth:  -0.1132075 Pred:  -0.0068419 Error:  -0.106365602036 Loss:  0.0600194866512\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0599761269644\n",
      "Truth:  -0.04716982 Pred:  0.00200423 Error:  -0.0491740457383 Loss:  0.0599680596894\n",
      "Truth:  -0.04716982 Pred:  0.00200423 Error:  -0.0491740457383 Loss:  0.0599600044551\n",
      "Truth:  -0.03773585 Pred:  0.00152074 Error:  -0.0392565887041 Loss:  0.0599445656664\n",
      "Truth:  -0.09433963 Pred:  -0.00407076 Error:  -0.0902688693178 Loss:  0.0599671620178\n",
      "Truth:  -0.05660377 Pred:  0.00200423 Error:  -0.0586079957383 Loss:  0.0599661499804\n",
      "Truth:  0.3207547 Pred:  0.00200423 Error:  0.318750474262 Loss:  0.0601586978407\n",
      "Truth:  -0.05660377 Pred:  0.00200423 Error:  -0.0586079957383 Loss:  0.0601575449023\n",
      "Truth:  -0.0754717 Pred:  -0.00128979 Error:  -0.0741819092709 Loss:  0.0601679641924\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0601247839856\n",
      "Truth:  0.1792453 Pred:  0.00200423 Error:  0.177241074262 Loss:  0.0602116655066\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0601685169226\n",
      "Truth:  0.03773585 Pred:  0.00200423 Error:  0.0357316242617 Loss:  0.0601504155207\n",
      "Truth:  0.1037736 Pred:  0.00200423 Error:  0.101769374262 Loss:  0.0601812215597\n",
      "Truth:  0.2169811 Pred:  0.00881165 Error:  0.20816944734 Loss:  0.0602906803066\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0602476008871\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0602045851004\n",
      "Truth:  -0.06603774 Pred:  -0.000204046 Error:  -0.0658336944417 Loss:  0.0602087394247\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.060165815742\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.060122955322\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0600801580248\n",
      "Truth:  -0.02830189 Pred:  0.00200423 Error:  -0.0303061157383 Loss:  0.0600582492372\n",
      "Truth:  -0.0754717 Pred:  0.00200423 Error:  -0.0774759257383 Loss:  0.0600710563523\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0600283915245\n",
      "Truth:  -0.09433963 Pred:  -0.000568833 Error:  -0.0937707970307 Loss:  0.0600531656842\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0600105765867\n",
      "Truth:  -0.09433963 Pred:  -0.0307465 Error:  -0.0635931737644 Loss:  0.0600132031242\n",
      "Truth:  -0.08490566 Pred:  -0.000913528 Error:  -0.0839921315073 Loss:  0.0600307701047\n",
      "Truth:  0.0 Pred:  0.000752414 Error:  -0.000752414343879 Loss:  0.0599873745295\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0599587606093\n",
      "Truth:  0.3773585 Pred:  0.00200423 Error:  0.375354274262 Loss:  0.0601893128854\n",
      "Truth:  -0.03773585 Pred:  0.00200423 Error:  -0.0397400757383 Loss:  0.0601743755318\n",
      "Truth:  -0.1792453 Pred:  -0.0171641 Error:  -0.16208115161 Loss:  0.0602487600398\n",
      "Truth:  -0.0754717 Pred:  0.00200423 Error:  -0.0774759257383 Loss:  0.0602613254415\n",
      "Truth:  -0.05660377 Pred:  0.00200423 Error:  -0.0586079957383 Loss:  0.0602601203907\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0602176907515\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.060175322873\n",
      "Truth:  -0.08490566 Pred:  -0.0108652 Error:  -0.0740404894914 Loss:  0.0601854066306\n",
      "Truth:  -0.1320755 Pred:  -0.0153453 Error:  -0.116730231117 Loss:  0.060226500253\n",
      "Truth:  -0.03773585 Pred:  -0.0079129 Error:  -0.0298229524951 Loss:  0.0602044206976\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0601621854328\n",
      "Truth:  -0.1886793 Pred:  -0.0068516 Error:  -0.18182770138 Loss:  0.060250412783\n",
      "Truth:  -0.08490566 Pred:  0.00200423 Error:  -0.0869098857383 Loss:  0.0602697312417\n",
      "Truth:  -0.08490566 Pred:  0.00200423 Error:  -0.0869098857383 Loss:  0.0602890217228\n",
      "Truth:  -0.03773585 Pred:  0.00200423 Error:  -0.0397400757383 Loss:  0.0602741527315\n",
      "Truth:  -0.05660377 Pred:  0.00200423 Error:  -0.0586079957383 Loss:  0.0602729479904\n",
      "Truth:  -0.03773585 Pred:  0.00200423 Error:  -0.0397400757383 Loss:  0.0602581121\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0602160515323\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.060174051658\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0601321123458\n",
      "Truth:  0.0 Pred:  -0.0143694 Error:  0.0143693806604 Loss:  0.060099142078\n",
      "Truth:  -0.06603774 Pred:  0.00132778 Error:  -0.0673655165829 Loss:  0.0601043734492\n",
      "Truth:  -0.04716982 Pred:  0.00200423 Error:  -0.0491740457383 Loss:  0.0600965099041\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0600547469392\n",
      "Truth:  0.3113208 Pred:  0.00200423 Error:  0.309316574262 Loss:  0.0602338143439\n",
      "Truth:  0.009433962 Pred:  0.00200423 Error:  0.00742973626165 Loss:  0.0601959076116\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0601676983132\n",
      "Truth:  -0.1509434 Pred:  0.00142868 Error:  -0.152372082706 Loss:  0.0602337946461\n",
      "Truth:  -0.03773585 Pred:  0.000813795 Error:  -0.0385496446716 Loss:  0.0602182615874\n",
      "Truth:  -0.03773585 Pred:  0.00111071 Error:  -0.038846562532 Loss:  0.0602029633061\n",
      "Truth:  -0.03773585 Pred:  0.00200423 Error:  -0.0397400757383 Loss:  0.0601883260474\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0601467362688\n",
      "Truth:  -0.03773585 Pred:  0.00200423 Error:  -0.0397400757383 Loss:  0.0601321600827\n",
      "Truth:  0.254717 Pred:  0.00200423 Error:  0.252712774262 Loss:  0.060269619479\n",
      "Truth:  -0.06603774 Pred:  0.00112569 Error:  -0.0671634305273 Loss:  0.0602745366053\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.0602330039532\n",
      "Truth:  -0.0754717 Pred:  0.00200423 Error:  -0.0774759257383 Loss:  0.0602452852365\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0602172616497\n",
      "Truth:  -0.1509434 Pred:  0.00200423 Error:  -0.152947625738 Loss:  0.0602832149669\n",
      "Truth:  -0.1886793 Pred:  0.000196353 Error:  -0.188875653067 Loss:  0.0603746097346\n",
      "Truth:  0.0 Pred:  0.00200423 Error:  -0.00200422573835 Loss:  0.060333153496\n",
      "Truth:  -0.04716982 Pred:  -0.0485748 Error:  0.00140500388616 Loss:  0.0602913308206\n",
      "Truth:  -0.1132075 Pred:  -0.00326168 Error:  -0.109945820682 Loss:  0.0603265467708\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0602985847574\n",
      "Truth:  -0.01886792 Pred:  0.00200423 Error:  -0.0208721457383 Loss:  0.0602706623501\n",
      "Truth:  -0.05660377 Pred:  0.00200423 Error:  -0.0586079957383 Loss:  0.0602694856575\n"
     ]
    }
   ],
   "source": [
    "loss = 0\n",
    "for i in range(len(X_validation)):\n",
    "    val = X_validation[i]\n",
    "    y = y_validation[i]\n",
    "    imgs = []\n",
    "    imgs.append(process_line({'path':val, 'steering':y})[0])\n",
    "    pred = model.predict(np.array(imgs))\n",
    "    loss += abs(y - pred[0][0])\n",
    "    print('Truth: ', y, 'Pred: ', pred[0][0], 'Error: ', (y - pred[0][0]), 'Loss: ', loss/(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "    \n",
    "model.save('model.h5')\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from keras.models import load_model, model_from_json\n",
    "\n",
    "#json_file = open('model.json', 'r')\n",
    "#loaded_model_json = json_file.read()\n",
    "#json_file.close()\n",
    "#loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "#model.load_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for layer in model.layers:\n",
    "#    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#json_file = open('model.json', 'r')\n",
    "#loaded_model_json = json_file.read()\n",
    "#json_file.close()\n",
    "#loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "#model.load_weights(\"model1.h5\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
