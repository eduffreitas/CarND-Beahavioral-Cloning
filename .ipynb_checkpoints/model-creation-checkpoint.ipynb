{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CarND Behavioral Cloning Project\n",
    "\n",
    "This project is about training a neural network to drive a car on a simulator using data recorded from a humman driver.\n",
    "\n",
    "This notebook will be used to create the model to be used in driving the car on the simulator.\n",
    "\n",
    "The inputs come in three images right, central and left cameras.\n",
    "\n",
    "The first thing to do is to clean, then oganize the dataset, and save it to pickle file. for posterior use.\n",
    "\n",
    "The file driving_log.csv contains steering angles and the left, right and center images associated to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import csv\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os.path\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files exist\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if not(os.path.exists('train.csv') and os.path.exists('validation.csv')):\n",
    "\n",
    "    path_to_replace = \"C:\\\\Users\\\\eduardo\\\\Documents\\\\SelfDrivingCar\\\\beta-simulator-windows\\\\beta_simulator_windows\\\\data\"\n",
    "\n",
    "    def ReplaceWrongPath(value):\n",
    "        return value.replace(path_to_replace, \"\").replace(\"\\\\\", \"/\").replace(\" \", \"\")\n",
    "\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    X_train_left = []\n",
    "    y_train_left = []\n",
    "    X_train_right = []\n",
    "    y_train_right = []\n",
    "\n",
    "    with open('./data/driving_log.csv', 'r') as csv_file_in:\n",
    "\n",
    "        csv_reader = csv.DictReader(csv_file_in)\n",
    "\n",
    "        for row in csv_reader:\n",
    "            steering = float(row['steering'])\n",
    "\n",
    "            #center image\n",
    "            path = './data/' + ReplaceWrongPath(row['center'].strip())        \n",
    "            X_train.append(path)\n",
    "            y_train.append(steering)\n",
    "            continue\n",
    "            \n",
    "            if steering == 0:\n",
    "                continue\n",
    "                \n",
    "            if steering < 0:\n",
    "                #left image\n",
    "                path = './data/' + ReplaceWrongPath(row['left'].strip())\n",
    "                steering_left = steering + 0.2\n",
    "\n",
    "                X_train_left.append(path)\n",
    "                y_train_left.append(steering_left)\n",
    "\n",
    "                #right image\n",
    "                path = './data/' + ReplaceWrongPath(row['right'].strip())\n",
    "                steering_right = steering - 0.2\n",
    "                steering_right = steering_right if steering_right > -1 else -1\n",
    "\n",
    "                X_train_right.append(path)\n",
    "                y_train_right.append(steering_right)\n",
    "            else:\n",
    "                #left image\n",
    "                path = './data/' + ReplaceWrongPath(row['left'].strip())\n",
    "                steering_left = steering + 0.2\n",
    "                steering_left = steering_left if steering_left < 1 else 1\n",
    "\n",
    "                X_train_left.append(path)\n",
    "                y_train_left.append(steering_left)\n",
    "\n",
    "                #right image\n",
    "                path = './data/' + ReplaceWrongPath(row['right'].strip())\n",
    "                steering_right = steering - 0.2\n",
    "\n",
    "                X_train_right.append(path)\n",
    "                y_train_right.append(steering_right)\n",
    "\n",
    "    X_train, y_train = shuffle(X_train, y_train)\n",
    "    X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "    [X_train.append(item) for item in X_train_left]\n",
    "    [X_train.append(item) for item in X_train_right]\n",
    "    [y_train.append(item) for item in y_train_left]\n",
    "    [y_train.append(item) for item in y_train_right]\n",
    "    \n",
    "    X_train, y_train = shuffle(X_train, y_train)\n",
    "\n",
    "    with open('train.csv', 'w') as csv_file_train:\n",
    "\n",
    "        fieldnames = ['path','steering']\n",
    "        writer = csv.DictWriter(csv_file_train, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for i in range(len(X_train)):\n",
    "            path, steering = X_train[i], y_train[i]\n",
    "            writer.writerow({'path': path, 'steering': steering})\n",
    "\n",
    "    with open('validation.csv', 'w') as csv_file_train:\n",
    "\n",
    "        fieldnames = ['path','steering']\n",
    "        writer = csv.DictWriter(csv_file_train, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for i in range(len(X_validation)):\n",
    "            path, steering = X_validation[i], y_validation[i]\n",
    "            writer.writerow({'path': path, 'steering': steering})\n",
    "\n",
    "    print(\"processing done\")\n",
    "else:\n",
    "    print(\"files exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading done\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "def turn_linear_to_logistic(steering, n_classes):\n",
    "    interval = 2/n_classes\n",
    "    classes = []\n",
    "    lower_bound = -1\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        upper_bound = lower_bound + interval if i < (n_classes-1) else 1\n",
    "        classes.append(1 if steering > lower_bound and steering < upper_bound else 0)\n",
    "        lower_bound += interval\n",
    "        \n",
    "    return np.array(classes)\n",
    "    \n",
    "    \n",
    "break_classes = 21\n",
    "X_train_left = None\n",
    "y_train_left = None\n",
    "X_train_right = None\n",
    "y_train_right = None\n",
    "\n",
    "X_train= []\n",
    "y_train = []\n",
    "\n",
    "with open('train.csv', 'r') as csv_file_train:\n",
    "    \n",
    "    csv_reader = csv.DictReader(csv_file_train)\n",
    "\n",
    "    for row in csv_reader:\n",
    "        X_train.append(row['path'])\n",
    "        y_train.append(float(row['steering']))\n",
    "        \n",
    "\n",
    "X_validation = []\n",
    "y_validation = []\n",
    "\n",
    "with open('validation.csv', 'r') as csv_file_val:\n",
    "    \n",
    "    csv_reader = csv.DictReader(csv_file_val)\n",
    "\n",
    "    for row in csv_reader:\n",
    "        X_validation.append(row['path'])\n",
    "        y_validation.append(float(row['steering']))\n",
    "        \n",
    "print('Loading done')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi0AAAGHCAYAAAB8hmJnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f22c8dca6d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y_train, bins=70)\n",
    "plt.title(\"Training data\")\n",
    "plt.xlabel(\"Angles\")\n",
    "plt.ylabel(\"# of examples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "#### VGG with drop out, batch normalition\n",
    "\n",
    "#### 20 EPOCHS Adam optimizer linear regression\n",
    "\n",
    "#### Validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Lambda, Dense, Activation, Flatten, Input, Dropout, Convolution2D, MaxPooling2D, Cropping2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "def get_model():\n",
    "    input_tensor = Input(shape=(160, 320, 3))\n",
    "    croped_input_img = Cropping2D(cropping=((60, 20), (0, 0)))(input_tensor)\n",
    "    croped_input_img = Lambda(lambda x: (x / 255.0) - 0.5)(croped_input_img)\n",
    "    base_model = VGG16(input_tensor=croped_input_img, weights='imagenet', include_top=False)\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    top_model = base_model.output\n",
    "    top_model = Flatten()(top_model)\n",
    "\n",
    "    top_model = Dense(1024)(top_model)\n",
    "    top_model = BatchNormalization()(top_model)\n",
    "    top_model = Activation('relu')(top_model)\n",
    "    top_model = Dropout(0.5)(top_model)\n",
    "\n",
    "    top_model = Dense(100)(top_model)\n",
    "    top_model = BatchNormalization()(top_model)\n",
    "    top_model = Activation('relu')(top_model)\n",
    "    top_model = Dropout(0.5)(top_model)\n",
    "\n",
    "    top_model = Dense(50)(top_model)\n",
    "    top_model = BatchNormalization()(top_model)\n",
    "    top_model = Activation('relu')(top_model)\n",
    "    top_model = Dropout(0.5)(top_model)\n",
    "\n",
    "    top_model = Dense(10)(top_model)\n",
    "    top_model = BatchNormalization()(top_model)\n",
    "    top_model = Activation('relu')(top_model)\n",
    "    top_model = Dropout(0.5)(top_model)\n",
    "\n",
    "    predictions = Dense(1)(top_model)\n",
    "\n",
    "    model = Model(input=input_tensor, output=predictions)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import merge, ZeroPadding2D, Cropping2D, Convolution2D, MaxPooling2D, Input, Lambda, Flatten, Dense, Activation, Dropout\n",
    "from keras.models import Model\n",
    "\n",
    "def AddConvLayer(features, kernel, input_layer):\n",
    "    complex_layer = ZeroPadding2D((1,1))(input_layer)\n",
    "    complex_layer = Convolution2D(features, kernel[0], kernel[1], activation='relu')(complex_layer)\n",
    "    #complex_layer = Dropout(0.5)(complex_layer)\n",
    "    complex_layer = ZeroPadding2D((1,1))(complex_layer)\n",
    "    complex_layer = Convolution2D(features, kernel[0], kernel[1], activation='relu')(complex_layer)\n",
    "    #complex_layer = Dropout(0.5)(complex_layer)\n",
    "    complex_layer = MaxPooling2D((2,2), strides=(2,2))(complex_layer)\n",
    "    return complex_layer\n",
    "\n",
    "def AddDenseLayer(neurons, depth, layer):\n",
    "    for i in range(depth):\n",
    "        layer = Dense(neurons, activation='sigmoid')(layer)\n",
    "        layer = Dropout(0.5)(layer)\n",
    "    return layer\n",
    "\n",
    "def AddInceptionModule(input_img):\n",
    "    tower_1 = Convolution2D(64, 1, 1, border_mode='same', activation='relu')(input_img)\n",
    "    tower_1 = Convolution2D(64, 3, 3, border_mode='same', activation='relu')(tower_1)\n",
    "\n",
    "    tower_2 = Convolution2D(64, 1, 1, border_mode='same', activation='relu')(input_img)\n",
    "    tower_2 = Convolution2D(64, 5, 5, border_mode='same', activation='relu')(tower_2)\n",
    "\n",
    "    tower_3 = MaxPooling2D((3, 3), strides=(1, 1), border_mode='same')(input_img)\n",
    "    tower_3 = Convolution2D(64, 1, 1, border_mode='same', activation='relu')(tower_3)\n",
    "\n",
    "    output = merge([tower_1, tower_2, tower_3], mode='concat', concat_axis=1)\n",
    "    return output\n",
    "\n",
    "def get_model():\n",
    "    input_img = Input(shape=(160, 320, 3))\n",
    "    croped_input_img = Cropping2D(cropping=((50, 20), (0, 0)))(input_img)\n",
    "    croped_input_img = Lambda(lambda x: (x / 255.0) - 0.5)(croped_input_img)\n",
    "    \n",
    "    x = AddConvLayer(32, (5, 5), croped_input_img)\n",
    "    x = AddConvLayer(64, (5, 5), x)\n",
    "    x = AddConvLayer(128, (5, 5), x)\n",
    "    x = AddConvLayer(256, (3, 3), x)\n",
    "    x = AddConvLayer(512, (3, 3), x)\n",
    "    x = AddInceptionModule(x)\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    output = AddDenseLayer(1024, 1, x)\n",
    "    output = AddDenseLayer(84, 1, output)\n",
    "    output = AddDenseLayer(50, 1, output)\n",
    "\n",
    "    predictions = Dense(10, activation='sigmoid')(output)\n",
    "    predictions = Dropout(0.5)(predictions)\n",
    "    predictions = Dense(1)(predictions)\n",
    "    \n",
    "    model = Model(input=input_img, output=predictions)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from keras.layers import merge, ZeroPadding2D, Cropping2D, Convolution2D, MaxPooling2D, Input, Lambda, Flatten, Dense, Activation, Dropout\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "\n",
    "def AddDenseLayer(neurons, depth, layer):\n",
    "    for i in range(depth):\n",
    "        layer = Dense(neurons)(layer)\n",
    "        layer = BatchNormalization()(layer)\n",
    "        layer = Activation('sigmoid')(layer)\n",
    "        layer = Dropout(0.5)(layer)\n",
    "    return layer\n",
    "\n",
    "def get_model():\n",
    "    input_img = Input(shape=(160, 320, 3))\n",
    "    croped_input_img = Cropping2D(cropping=((50, 20), (0, 0)))(input_img)\n",
    "    croped_input_img = Lambda(lambda x: (x / 255.0) - 0.5)(croped_input_img)\n",
    "    \n",
    "    x = ZeroPadding2D((1,1))(croped_input_img)\n",
    "    x = Convolution2D(8, 5, 5)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)    \n",
    "    x = Dropout(0.5)(x)\n",
    "    x = MaxPooling2D((2,2), strides=(2,2))(x)\n",
    "    \n",
    "    x = ZeroPadding2D((1,1))(x)\n",
    "    x = Convolution2D(16, 5, 5)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = MaxPooling2D((2,2), strides=(2,2))(x)\n",
    "    \n",
    "    x = ZeroPadding2D((1,1))(x)\n",
    "    x = Convolution2D(32, 5, 5)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = MaxPooling2D((2,2), strides=(2,2))(x)\n",
    "    \n",
    "    x = ZeroPadding2D((1,1))(x)\n",
    "    x = Convolution2D(64, 3, 3)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = MaxPooling2D((2,2), strides=(2,2))(x)\n",
    "    \n",
    "    x = ZeroPadding2D((1,1))(x)\n",
    "    x = Convolution2D(128, 3, 3)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)    \n",
    "    x = Dropout(0.5)(x)\n",
    "    x = MaxPooling2D((2,2), strides=(2,2))(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "\n",
    "    output = AddDenseLayer(4064, 1, x)\n",
    "    output = AddDenseLayer(2032, 1, x)\n",
    "    output = AddDenseLayer(100, 1, x)    \n",
    "    predictions = Dense(1)(output)\n",
    "    \n",
    "    model = Model(input=input_img, output=predictions)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_line(row):\n",
    "    #b,g,r = cv2.split(cv2.imread(row['path']))\n",
    "    #img = np.array(cv2.merge([r,g,b]))\n",
    "    img = cv2.imread(row['path'])\n",
    "    steering = float(row['steering'])\n",
    "    return [img, steering]\n",
    "\n",
    "def generate_arrays_from_file(path, batch_size = 20, flip=True):\n",
    "    while 1:\n",
    "        global X_train\n",
    "        global y_train\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        Xs = []\n",
    "        ys = []        \n",
    "        for i in range(len(X_train)):\n",
    "            if (len(Xs) == batch_size):\n",
    "                yield (np.array(Xs), np.array(ys))\n",
    "                Xs = []\n",
    "                ys = []\n",
    "                \n",
    "            x, y = process_line({'path':X_train[i], 'steering':y_train[i]})\n",
    "            Xs.append(x)\n",
    "            ys.append(y)\n",
    "            \n",
    "            if flip:\n",
    "                if (len(Xs) == batch_size):\n",
    "                    yield (np.array(Xs), np.array(ys))\n",
    "                    Xs = []\n",
    "                    ys = []\n",
    "                    \n",
    "                x_flipped = np.fliplr(x)\n",
    "                y_filpped = -y\n",
    "\n",
    "                Xs.append(x_flipped)\n",
    "                ys.append(y_filpped)\n",
    "\n",
    "        yield (np.array(Xs), np.array(ys))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train records:  11815\n",
      "validation records:  2954\n"
     ]
    }
   ],
   "source": [
    "train_rows = len(X_train)\n",
    "validation_rows = len(X_validation)\n",
    "    \n",
    "print('train records: ', train_rows)\n",
    "print('validation records: ', validation_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "23600/23630 [============================>.] - ETA: 0s - loss: 0.5447Epoch 00000: val_loss improved from inf to 0.18945, saving model to model1.h5\n",
      "23630/23630 [==============================] - 699s - loss: 0.5445 - val_loss: 0.1894\n",
      "Epoch 2/30\n",
      "23600/23630 [============================>.] - ETA: 0s - loss: 0.3329Epoch 00001: val_loss improved from 0.18945 to 0.16499, saving model to model1.h5\n",
      "23630/23630 [==============================] - 670s - loss: 0.3328 - val_loss: 0.1650\n",
      "Epoch 3/30\n",
      "23600/23630 [============================>.] - ETA: 0s - loss: 0.2399Epoch 00002: val_loss improved from 0.16499 to 0.15378, saving model to model1.h5\n",
      "23630/23630 [==============================] - 670s - loss: 0.2398 - val_loss: 0.1538\n",
      "Epoch 4/30\n",
      "23600/23630 [============================>.] - ETA: 0s - loss: 0.1978Epoch 00003: val_loss improved from 0.15378 to 0.15198, saving model to model1.h5\n",
      "23630/23630 [==============================] - 670s - loss: 0.1979 - val_loss: 0.1520\n",
      "Epoch 5/30\n",
      "23600/23630 [============================>.] - ETA: 0s - loss: 0.1775Epoch 00004: val_loss improved from 0.15198 to 0.14985, saving model to model1.h5\n",
      "23630/23630 [==============================] - 671s - loss: 0.1774 - val_loss: 0.1498\n",
      "Epoch 6/30\n",
      "23600/23630 [============================>.] - ETA: 0s - loss: 0.1675Epoch 00005: val_loss improved from 0.14985 to 0.14183, saving model to model1.h5\n",
      "23630/23630 [==============================] - 669s - loss: 0.1675 - val_loss: 0.1418\n",
      "Epoch 7/30\n",
      "23600/23630 [============================>.] - ETA: 0s - loss: 0.1595Epoch 00006: val_loss improved from 0.14183 to 0.13503, saving model to model1.h5\n",
      "23630/23630 [==============================] - 669s - loss: 0.1595 - val_loss: 0.1350\n",
      "Epoch 8/30\n",
      "23600/23630 [============================>.] - ETA: 0s - loss: 0.1520Epoch 00007: val_loss improved from 0.13503 to 0.13462, saving model to model1.h5\n",
      "23630/23630 [==============================] - 669s - loss: 0.1520 - val_loss: 0.1346\n",
      "Epoch 9/30\n",
      "23600/23630 [============================>.] - ETA: 0s - loss: 0.1454Epoch 00008: val_loss improved from 0.13462 to 0.12660, saving model to model1.h5\n",
      "23630/23630 [==============================] - 669s - loss: 0.1454 - val_loss: 0.1266\n",
      "Epoch 10/30\n",
      "23600/23630 [============================>.] - ETA: 0s - loss: 0.1399Epoch 00009: val_loss improved from 0.12660 to 0.11898, saving model to model1.h5\n",
      "23630/23630 [==============================] - 669s - loss: 0.1398 - val_loss: 0.1190\n",
      "Epoch 11/30\n",
      "23600/23630 [============================>.] - ETA: 0s - loss: 0.1369Epoch 00010: val_loss improved from 0.11898 to 0.11386, saving model to model1.h5\n",
      "23630/23630 [==============================] - 669s - loss: 0.1369 - val_loss: 0.1139\n",
      "Epoch 12/30\n",
      "23600/23630 [============================>.] - ETA: 0s - loss: 0.1354Epoch 00011: val_loss did not improve\n",
      "23630/23630 [==============================] - 669s - loss: 0.1354 - val_loss: 0.1158\n",
      "Epoch 13/30\n",
      "23600/23630 [============================>.] - ETA: 0s - loss: 0.1330Epoch 00012: val_loss improved from 0.11386 to 0.10828, saving model to model1.h5\n",
      "23630/23630 [==============================] - 669s - loss: 0.1331 - val_loss: 0.1083\n",
      "Epoch 14/30\n",
      "23600/23630 [============================>.] - ETA: 0s - loss: 0.1328Epoch 00013: val_loss did not improve\n",
      "23630/23630 [==============================] - 669s - loss: 0.1327 - val_loss: 0.1123\n",
      "Epoch 15/30\n",
      "  300/23630 [..............................] - ETA: 605s - loss: 0.1283"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model = get_model()\n",
    "optimizer = RMSprop(lr=0.0001)\n",
    "model.compile(loss='mean_absolute_error', optimizer=optimizer)\n",
    "checkpoint = ModelCheckpoint('model1.h5', monitor='val_loss', verbose=1, save_best_only=True,\n",
    "                                 save_weights_only=False, mode='auto')\n",
    "history = model.fit_generator(\n",
    "    generate_arrays_from_file('train.csv',batch_size=100), \n",
    "    samples_per_epoch=train_rows*2, \n",
    "    nb_epoch=30, \n",
    "    validation_data=generate_arrays_from_file('validation.csv', batch_size=10, flip=True),\n",
    "    nb_val_samples=validation_rows*2, \n",
    "    callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = 0\n",
    "for i in range(len(X_validation)):\n",
    "    val = X_validation[i]\n",
    "    y = y_validation[i]\n",
    "    imgs = []\n",
    "    imgs.append(process_line({'path':val, 'steering':y})[0])\n",
    "    pred = model.predict(np.array(imgs))\n",
    "    loss += abs(y - pred[0][0])\n",
    "    print('Truth: ', y, 'Pred: ', pred[0][0], 'Error: ', (y - pred[0][0]), 'Loss: ', loss/(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for layer in model.layers:\n",
    "#    layer.name.encode('utf8')\n",
    "    \n",
    "    \n",
    "#model.save('model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from keras.models import load_model, model_from_json\n",
    "\n",
    "#json_file = open('model.json', 'r')\n",
    "#loaded_model_json = json_file.read()\n",
    "#json_file.close()\n",
    "#loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "#model.load_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for layer in model.layers:\n",
    "#    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#model = get_model()\n",
    "#json_file = open('model.json', 'r')\n",
    "#loaded_model_json = json_file.read()\n",
    "#json_file.close()\n",
    "#loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "#model.load_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
